% !TEX ROOT = ./distributed_mrf.tex
\section{Related work}
\label{sec:related-work}

\noindent%
\textbf{Superlinear scaling.} %
Amdahl's famous scaling law \cite{10.1145/1465482.1465560}, asserting sublinear speedup and diminishing returns for parallelization, is a cornerstone result in distributed computing \cite{10.1145/42411.42415, 10.5555/775339.775386,1580395, 10.5555/1951599, 10.1145/42411.42415}. During the almost 60 years since its first publication various use cases were reported that seemingly violate Amdahl's scaling, triggering several useful extensions of the basic law \cite{4563876, 6280307, 406581, 6163449, 10.5555/1951599}. One such phenomenon is faster-than-linear scaling, observed in a broad range of production workloads \cite{scalability-analyzed, 10.1145/3627703.3629574, 10.5555/1012889.1012894, sdn-analytitcs, 556383, 7733347, 6483679, 10.1007/978-3-319-77610-1,dobb-1, dobb-2}. For instance, \cite{scalability-analyzed} shows superlinear speedup for PostgreSQL and traces back the reason to a new ``cache plan'' for caching compiled SQL queries at each thread, \cite{7733347} shows that dense matrix multiplication may exhibit faster-than-linear speedup when matrix rows\slash columns are optimized for CPU caches, etc. Superlinear growth is often found in Nature as well, e.g., describing the scaling of human communities to large cities \cite{PhysRevE.79.016115}.

There seem to be two common strategies to obtain superlinear scaling \cite{7733347, 80148}: either do disproportionately less work per worker as system is scaled \cite{7733347}, or add more resources per thread \cite{80148}. These techniques, however, are difficult to apply beyond specific use cases \cite{556383} or require adding more cache space \cite{7733347}. Meanwhile, there have been heated debates on the controversies related to superlinear scaling: for instance, Gunther shows that an earlier report on faster-than-linear scaling from a Hadoop MapReduce workload is attributable to a benchmarking error and, when measured the right way, reduces to sublinear scaling \cite{10.1016/0167-8191(86)90024-4}. % This prompts the authors to conclude that superlinearity is ultimately a performance illusion, which goes against the very laws of thermodynamics.
To the best of our knowledge, ours is the first general methodology that can systematically reproduce superlinear growth in a broad range of applications.

\noindent%
\textbf{Locality-boosting load balancing.} %
In line with the recent trend to leverage NICs for intelligently moving data between the network, CPU, GPU and accelerators in computing systems \cite{sherry-ccr23}, there have been several efforts to extend the static hash-based load balancing provided by RSS: Receive Flow Steering (RFS) is a mechanism to steer flows to the CPU on which the application that processes the flow is running \cite{rss-linux} and RSS++ is a dynamic receive side scaling mechanism aiming to keep CPU load constant \cite{10.1145/3359989.3365412}. These mechanisms could be leveraged to implement more efficient locality boosting in the NIC: RFS can be used to direct all flows matching the same rule to the same CPU, RSS++ could be used to evenly spread load even for staggering workers that process the ``difficult'' high-dependency rules, etc. Furthermore, Reframer can be used to reorder packets for improving the temporal locality at workers' input \cite{276946,246322}, and SAX-PAC can be used to decompose a classifier rule set with many dependencies into multiple smaller but independent rule sets \cite{10.1145/2619239.2626294}.  Hicuts \cite{820051}, Hypercuts \cite{10.1145/863955.863980}, Efficuts \cite{10.1145/1851182.1851208}, and CutSplit \cite{8485947} define ``intelligent'' packet header space cuts \cite{180587} to partition a rule set along a decision tree into smaller rule lists stored in the leaves of the tree. These schemes are complimentary to our approach: while \cite{820051, 10.1145/863955.863980, 10.1145/1851182.1851208, 8485947} use ``smart'' cuts with ``dumb'' lists in the leaves we rather use ``dumb'' cuts, implemented by hash-based load balancing, with ``smart'' rule lists in the workers to reach superlinear parallel scaling.

\noindent%
\textbf{Self-adjusting data structures.} %
Self-adjusting algorithms, the other ingredient for superlinear scaling, are widely applied in computer systems: caches are extensively used in predictive NFV state stores \cite{295537}, database accelerators \cite{10.5555/1012889.1012894, 180324, ghigoff2021bmc}, distributed web caching and CDNs \cite{295603}, and microservices \cite{295493}; move-to-front (MTF) lists are used for computing point maxima and convex hulls \cite{BentleyCL93}, program compilation and interpretation \cite{HesterH85}, detecting collisions in hash tables \cite{HesterH85}, and data compression \cite{BentleySTW86}; further examples are splay trees \cite{SleatorT85Splay}, self-adjusting skip lists \cite{BoseDL08}, push-down trees \cite{Avin0020}, or self-adjusting geometric data stores \cite{ParkM12}, etc. Another example for self-adjustment are runtime optimization frameworks which can just-in-time recompile code to specialize it to a particular structured input \cite{276946,246322,10.1145/3503222.3507769,procieee_2019}.  All these are candidates to be used, along with a proper locality-boosting load balancer, to reach superlinear scaling in distributed applications.  To what extent these algorithms \emph{already} achieve superlinear scaling in production applications is perhaps one of the most intriguing open questions for future research.
  
%   Models of self-adjusting data structures are based on the cost of access and rearrangement. For example:

%   \begin{enumerate}
%   \item \textbf{Self-adjusting lists}~\cite{SleatorT85}. We are given a set of items, arranged in a linear list, and a sequence of access requests $\sigma$ to the nodes of the list.
%     Upon receiving an access request to a node in
%     the list, an algorithm searches linearly through the list, starting
%     from the head of the list, traversing nodes until encountering the
%     accessed node. Accessing the node at position i in the list costs i
%     (the first node is at position 1).
%     After serving a request, an algorithm may
%     choose to rearrange the nodes of the list, paying the cost 1 per each transposition of neighboring items. 


%   \item \textbf{Binary search trees}~\cite{SleatorT85Splay}.
%     When the universe of items is ordered, we may store them in a binary search tree.
%     A classic binary search tree is a \emph{splay tree}~\cite{SleatorT85Splay}.
%     Another important search tree is $O(\log \log n)$-competitive \emph{tango tree}~\cite{demaine2007dyynamic}.
%     The dynamic optimality conjecture~\cite{SleatorT85Splay} (does an $O(1)$-competitive algorithm exist?) is a major unresolved question, in contrast to the simpler self-adjusting lists setting.
%     Splay trees have other properties, e.g. working set bounds, static optimality~\cite{SleatorT85Splay} and other.



%   \item \textbf{Other self-adjusting data structures}. 
%     Self-adjusting skip lists~\cite{BoseDL08} have an equivalent of the working set bound of splay trees.
%     Push-down trees~\cite{Avin0020} are dynamically optimal and have the working set bound.
%     Adaptive geometric space partitioning data structures exist, e.g. self-adjusting trees for storing geometric data~\cite{ParkM12}.
%     The online metrical task system model~\cite{Borodin1992} underpins all these models, and captures generalizations such as caching, which has self-adjusting algorithms such as LRU~\cite{SleatorT85}.
%   \end{enumerate}


% %   Other examples: intrusion detection as mtflist, flow table lookup as splay tree, etc.
% %   Each have their own challenges, and our model is just an example.

%   \paragraph*{Locality.}
%   Common inputs have high locality, i.e. the same items are accessed repeatedly.
%   The locality parameter of input is often the determining factor for the performance of self-adjusting data structures (e.g. there exist arguments of locality for self-adjusting lists~\cite{AlbersL16}, working set bounds for splay trees~\cite{SleatorT85Splay} and paging~\cite{AlbersFG05}).


%   \subsection{Load Balancing and Scaling}

%   \paragraph*{Load balancing with random hash functions.}
%   A random load balancing assignment function is sufficient to load-balance correctly.
%   Gonnet~\cite{Gonnet81} proved that when throwing $n$ balls uniformly and independently at random into $n$ bins, the fullest bin has
%   load $(1 + o(1)) \log n/ \log \log n$ in expectation.
%   The maximum bin load with this approach is $O(\log n/ \log \log n)$ with high probability~\cite{DubhashiR98}.

%   \paragraph*{Practical load balancing.}
%   RSS+ paper~\cite{10.1145/3359989.3365412}.

%   \subsection{Packet classification}

%   Various data structures for packet classification were proposed in the literature: lists, tries, hash tables, bit vectors, or decision trees~\cite{gupta2001algorithms,Srinivasan1999,Eppstein2001}, as well as hardware solutions (TCAM).
%   Packet classifiers are often accompanied by caching systems that provide some adjustability to traffic.
%   Due to its simplicity, a~linear lookup structure is commonly applied in practice, e.g., in the default firewall suite of the Linux operating system kernel called \texttt{iptables}~\cite{MianoBRBLP19}, the OpenFlow reference switch~\cite{openflow}, and in many QoS classifiers.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

