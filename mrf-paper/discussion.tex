\section{Discussion/Limitations}
However, as noted earlier, this design pattern achieves superlinear scaling only under certain conditions and is therefore not universally applicable. In this section, we outline these conditions and share our experience in applying the pattern to engineer systems that realize superlinear scaling, distilled into a set of design guidelines:
(1) \emph{Optimize the load balancer for the worker implementation} (or the other way around) so that the load balancer boosts exactly the type of locality workers can exploit. A load balancer that boosts temporal locality will not improve the performance of a worker designed for spatial locality (and \emph{vice versa}). (2) \emph{Make workers' internal data structures independent} so that each worker can autonomously rearrange itself with respect the locality of its own input. Shared data structures will not work. For instance, \cite{ghigoff2021bmc} maintains a single cache to offload popular Memcached queries that is shared across kernel threads. This blocks parallel self-adjustment by (re)mixing the locality in the threads' input into a single unstructured workload. (3) \emph{Workers must be CPU-bounded}, otherwise the system cannot benefit from parallelization. In many cases bounded memory is also needed for self-adjustment to count (e.g., in distributed caching). (4) \emph{Avoid sequential bottlenecks} that may block speedup prematurely. For instance, we experimented with running the load-balancer for our packet classifier in the Linux kernel's RPS function, which allows better locality boosting by letting us fine-tune the partitioning function, instead of the hardware RSS that supports only hash-based load balancing. Unfortunately, the single kernel thread quickly posed a firm sequential bottleneck, well before superlinear speedup could appear.