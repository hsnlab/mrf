\section{Limitations}
\label{sec:discussion}

Our experimental evaluations indicate that superlinear scaling occurs only under particular circumstances. In this section, we describe those conditions and share practical lessons from our own work, culminating in a set of design guidelines.

\noindent%
\textbf{Optimize the load balancer for the worker implementation} (or the other way around) so that the load balancer boosts exactly the type of locality workers can exploit. A load balancer that boosts temporal locality will not improve the performance of a worker designed for spatial locality (and \emph{vice versa}). Similarly, a load balancer splitting traffic by hashing on the IP source-destination pair will work for an L3 classifier that filters only on the network layer header fields, but it may cease to be properly ``partitioning'' if the classifier considers transport layer header fields as well. Properly matching the load balancer to the self-adjusting worker remains a laborious manual task for the moment.

\noindent%
\textbf{Make workers' internal data structures independent} so that each worker can autonomously rearrange itself with respect the locality of its own input. Shared data structures will not work. For instance, \cite{ghigoff2021bmc} maintains a single cache to offload popular Memcached queries that is shared across kernel threads. This blocks parallel self-adjustment by (re)mixing the locality in the threads' input into a single unstructured workload for the shared cache. In contrast, our MRF classifier implementation carefully allocates the per-worker rule-lists as private per-CPU pointer arrays, so that each worker can maintain its own rule order on top of the shared static rule list.

\noindent%
\textbf{Avoid sequential bottlenecks} that may block parallel speedup prematurely. We faced this problem during the design of our self-adjusting packet classifier. In order to improve the locality-boosting property of the load balancer we experimented with running it in the Linux kernel's RPS function, which admits more flexible traffic splitting rules than plain hardware RSS. This would result better locality boosting by letting us fine-tune the partitioning function for the classifier rules, in contrast to the hardware RSS that supports only hash-based load balancing. Despite that we identified substantial improvement at low packet rates, the single RPS kernel thread quickly posed a firm sequential bottleneck, blocking further scaling well before superlinear speedup could appear.

\noindent%
\textbf{Make self-adjustment matter} so that workers can take advantage of improved locality. If in a distributed caching system cached access is only moderately faster than uncached access then very little gain can be obtained from improving workers' cache efficiency. Likewise, if $k$ parallel caches are enough to cache the entire workload, adding a new worker will have little effect. For attaining super-linear speedup workers in a distributed self-adjusting system must be CPU-bound (so that adding parallel CPU resources matter) and, for caching in particular, also memory-bound (so that additional fast cache resources will translate into faster parallel execution). This is clearly the case for the self-adjusting classifier, as evidenced by our benchmarks.

\noindent%
\textbf{Choose an adequate baseline} for measuring speedup. An inadequate benchmarking methodology may easily ruin a faster-than-linear scaling trend or, what is worse, show a spurious superlinear speedup \cite{10.1145/2773212.2789974}. In this paper we report speedup with respect to the single-core non-self-adjusting baseline (in line with \eqref{eq:amdahl}). Comparing to the self-adjusting baseline superlinear scaling often disappears. To get a full picture, analyze speedup and raw performance results side-by-side. This allows to spot cases of spurious speedup resulting from a slow baseline.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf.nsdi"
%%% End:

