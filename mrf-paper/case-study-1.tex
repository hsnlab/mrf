% !TEX ROOT = ./distributed_mrf.tex
\section{Superlinear scaling in distributed caching}\label{sec:dist-caching}

The most prominent example for the scaled size model is \emph{distributed caching} \cite{scalability-analyzed, sdn-analytitcs, dobb-2} (for complete taxonomies see \cite{556383, 7733347, 80148}).  Most modern CPUs come with unshared Level-1 fast cache memory: the more CPU cores, the more fast memory is available for caching, which improves the cache-hit rate at the workers. This tends to speed up memory\slash cache-bound code disproportionately. Many distributed applications also contain a fast-path\slash cache; e.g., \texttt{memcached} is often used as a fast cache for a ``slow'' web service \cite{180324,10.5555/1012889.1012894}, popular keys are cached in the OS kernel for fast key-value store access \cite{179747, ghigoff2021bmc}, FIB caches maintain the most recent IP routes to sidestep longest prefix matching \cite{rottenstreich2016optimal}, hierarchical flow caches serve as a fast-path in programmable software switches \cite{188960}, etc. All these workloads may benefit from the caches becoming more efficient as the system is scaled and, potentially, show superlinear speedup on certain workloads. % We stress, however, that faster-than-linear speedup is strictly contingent on the way work is distributed across workers so that subproblem sizes indeed reduce, otherwise cache efficiency remains constant and superlinear growth vanishes (see later).

\begin{figure}
  \centering
  \begin{small}
    \input{fig/cache/cache_analysis.tex}
\end{small}
\caption{Scaling laws for distributed caching: hash-based load balancing, lower envelope (round robin load balancing) and upper envelope (perfect cache hit rate with $k$ caches). }
  \label{fig:dcache-analysis}
\end{figure}

% refer to "Modeling Speedup (n) Greater than n" -> analysis

% assumption for the analysis? ``I think equation 2 should be explained much better. It is not at all obvious (not sure even correct) that hit rate scales linearly with threads (I think only true is delta << 1). What is $\rho$ exactly the ratio of (fetching time in the event of miss) / (fetching time in the event of a catch hit)?''

It is instructive to quantify superlinear speedup in this context using a simple model. Suppose a source emits uniformly distributed random requests for $m$ items and requests are distributed among $k$ workers, each using a separate cache of size $c$, by hashing on the request id.  Initially, the cache hit rate for a single worker that processes all $m$ possible requests is $\delta := \sfrac{c}{m}$. Adding $k$ workers effectively partitions the requests into $k$ random buckets so that each worker will perceive uniformly distributed requests for only $\sfrac{m}{k}$ items, which improves the cache hit rate at each worker to $\frac{c}{\sfrac{m}{k}} = k\delta$ ($k\delta \le 1$). This puts the lookup time of the system of $k$ parallel caches to
\begin{align}\label{eq:dist-cache}
  T_c(k) = \begin{cases} s + \frac{1-s}{k}(k\delta + (1-k\delta)\rho) & \text{if } k\delta \le 1\\s + \frac{(1-s)}{k} & \text{otherwise}\end{cases} \enspace ,
\end{align}
where $\rho$ is the penalty for a cache miss event, $\delta$ is the cache hit rate for a single worker, and $s$ denotes the fraction of execution time spent in the sequential part of the code.


The speedup $S_c(k)=\frac{T_c(1)}{T_c(k)}$ for the parameters $s=0.1$, $\delta=0.1$ and $\rho=10$ is depicted in Fig.\ref{fig:dcache-analysis}. The lower envelope of the scaling profile is given by Amdahl's law for the system with random or round robin load-balancing. % ($\frac{T_c(1)}{s + \frac{1-s}{k}(\delta + (1-\delta)\rho)}$).
As $k$ grows the scaling profile progresses over a superlinear curve to an elevated Amdahl's law profile, representative of a system serving \emph{all} requests from fast memory. % ($\frac{T_c(1)}{s + \frac{1-s}{k}}$).
Note that this occurs \emph{only} if request dispatching is chosen carefully to partition the item space. Modulo hashing assigns the same item to the same worker deterministically, so that workers process only a subset of the items that may have a greater chance to fit into the cache. In contrast, a random or a round robin load balancer may assign any item to any worker, which defeats the purpose of improving workers' cache hit rate. % and destroys superlinear scaling all together. % (see empirical evidence in the next Section).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:
