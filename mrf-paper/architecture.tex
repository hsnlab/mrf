\section{Distributed self-adjusting systems}
\label{sec:architecture}

Next we show a general distributed systems architecture, which, as we show theoretically and empirically later, produces faster-than-linear scaling in several disparate problem domains. Our main observation is that, whenever genuinely observed, superlinear scaling assumes two critical components: a policy to dispatch jobs to workers in a way to increase the locality of reference in the per-worker input streams, plus an algorithm that can adaptively exploit the structure in the input to process it more efficiently. Our architecture is purely a software technique in that it does not require the addition of new cache space to a system. Nonetheless, it contains distributed caching as a special case and hence automatically takes advantage of additional fast memory, if available.

\subsection{Locality-boosting load balancing}
\label{sec:lb-lb}

The first crucial component in our architecture is a locality-boosting load balancer.  In this context, load balancing refers to the distribution of computational work or incoming network traffic across multiple parallel workers (servers, processors, or nodes). A good load balancing strategy ensures optimal resource utilization, minimizes response time, avoids overloading any single resource, and, as we argue below, improves the locality in the input presented to the workers. 

In the context of this paper, \emph{locality of reference} is the property of a sequence of inputs that subsequent items are statistically dependent on each other. Such structure in the input can then be readily exploited by the proper algorithm \cite{SleatorT85Splay, BentleyCL93, HesterH85, HesterH85, BentleySTW86, Avin0020, ParkM12} or a runtime optimization framework \cite{276946,246322,10.1145/3503222.3507769,procieee_2019} to improve the performance of the code that processes it. A request set with minimal locality is uniformly distributed on the entire domain of possible inputs and hence unpredictable, while one with maximal locality contains only a single item, i.e., maximally predictable. A \emph{locality-boosting load balancing} policy is then a request dispatching strategy that can statistically or deterministically improve the locality of reference experienced by the worker threads, \emph{turning an unpredictable system input into multiple streams of predictable inputs to be processed by the workers} (see Fig.~\ref{fig:locality-boosting-lb}).

\begin{figure}
  \centering
  \includegraphics[width=.85\linewidth]{fig/schema.pdf}
  \caption{A locality boosting load balancer partitions the input sequence of a given locality into subsequences with higher locality. Self-adjusting data structures perform better on inputs with higher locality.}
  \label{fig:locality-boosting-lb}
\end{figure}

We distinguish two types of locality in this context. \emph{Spatial locality} means that the distribution of requested items on the entire input domain is statistically biased towards a particular subset of the items. One way to ensure this in the load balancer is to \emph{partition} the input domain into disjunct subsets, so that worker's input distributions are concentrated on a smaller set of inputs. For instance, the hash-based load balancer we used previously to show superlinear scaling with distributed caching is such a partitioning load balancer. In contrast, a round robin or a uniform random load balancer will export its own spatial input locality unchanged to the workers. A related concept is \emph{temporal locality}, which refers to the reuse of specific items in the input within a relatively small time duration. One way to boost temporal locality is to reorder items within a time window: e.g., Reframer applies controlled delays to particular packets in a packet batch to boost temporal locality and, thereby, enable more efficient processing \cite{276946,246322}.

\subsection{Self-adjusting algorithms}
\label{sec:sa-alg}

The second critical enabler for superlinear scaling in our architecture is \emph{self-adjusting algorithms}. Self-adjustment is a general term referring to the property of a dynamic data structure to \emph{automatically reorganize itself based on the sequence of operations it receives}, in order to optimize performance for future operations on frequently accessed or manipulated elements. Internal data reorganization always introduces extra complexity and overhead compared to a static data structure. Therefore, self adjustment can improve performance only if the input processed by the algorithm exhibits a certain amount of spatial or temporal locality.

Next we review the most prominent self-adjusting data structures (but see also \cite{BoseDL08, Avin0020, ParkM12}).

% For further examples, see self-adjusting skip lists~\cite{BoseDL08}, push-down trees~\cite{Avin0020}, or geometric data storages \cite{ParkM12}.

\noindent%
\textbf{Caches.} %
As the simplest but most universal self-adjusting data structure, caches can serve frequently accessed items fast by storing them in a  software or a hardware fast memory. This is typically much faster than if we had to run the request through the full processing pipeline or the slow backing store. Thus, caches have that almost magical capability of self-adaptation, without us having to engineer any prior knowledge of the input into the cache mechanism apart from a promise that it has nontrivial locality. When the promise is true, caches are an inexpensive way to improve throughput and response time. When there is no locality in the input, however, caches usually just add extra latency and overhead.
%
Note that caches do not necessarily have to come in the form of hardware memory: a fast key-value store is a candidate cache for a slow database \cite{10.5555/1012889.1012894}, a kernel fast-path flow cache is a useful way to speed up a slow user-space software switch \cite{188960}, etc.

% However, caches also come with additional complexity and overhead: cache entries have to be created for storing recently accessed data, invalidated when the backing data changes, evicted when the cache is full, and synchronized to consistently represent data that may be present in multiple caches. % Hardware implementations are appealing from this aspect by hiding the extra complexity behind a fast on-chip implementation.

% The performance of a cache is determined by the cache hit rate $\delta$, defined as the ratio of operations served from fast memory to those served on the slow path, and $\rho$, the penalty of a cache miss. In general, for a common LFU or LRU cache the higher the locality in the input and the bigger the cache compared to the input domain, the higher the cache hit rate and the lower the response time. When there is no locality in the input, caches usually just add extra latency and overhead.

\noindent%
\textbf{List lookup.} %
One of the most widely used self-adjusting data structures is the \emph{move-to-front list}. Suppose we wish to store a list of $m$ items in a way so that reordering, insertion and deletion are fast, while lookup is also reasonably efficient. A straightforward choice is a static linked list. Here, the cost of accessing an item at position $i$ is exactly $i$. Then, any linked list can easily be upgraded to a self-adjusting list using the move-to-front (MTF) heuristics: after accessing an item it is moved to the front, which improves lookup time for future requests of the same item at minimal cost (see Fig.~\ref{fig:mtf-example}). The MTF heuristics comes with appealing theoretical properties, namely that unconditionally moving the accessed item to the front of the list is close to the best one could achieve, even if one knew all future requests \cite{SleatorT85}. MTF can handle both spatial and temporal locality. However, for uniformly distributed input MTF lists usually add nontrivial overhead compared to static lists due to the frequent and useless relinking of the list.

\begin{figure}
  \centering
  \includegraphics[width=.85\linewidth]{fig/mtf.pdf}
  \caption{A self-adjusting list containing nodes A,B,C and D serves the request to C and moves C to the front of the list to speed up future accesses to C.}
  \label{fig:mtf-example}
\end{figure}

Classic applications of MTF lists are information retrieval systems, compression~\cite{BentleySTW86}, etc. In general, any use case is a potential candidate application for MTF where the task is to match a request against a list of complex rules that do not lend themselves readily to be arranged into a fast lookup structure (e.g., a search tree), like inference in explainable rule-based AI % and expert systems
\cite{dovsilovic2018explainable}, rule matching in OpenFlow and P4 reference software switches \cite{openflow}, packet classification in networking (see later), etc.  We note that caching is a subset of list lookup, in that every algorithm for list reorganization gives rise to a different cache management algorithm \cite{SleatorT85}.  

\noindent%
\textbf{Search trees.} %
A search tree is an efficient tree data structure for locating specific keys from within an ordered set. A \emph{splay tree} is a self-adjusting version of a static search tree, in that it can dynamically reorganize itself by moving popular items closer to the root of tree and less frequently accessed elements to the bottom, while keeping the tree relatively well-balanced \cite{SleatorT85Splay, BoseDL08, Avin0020}. Since access time in a search tree is determined by the depth at which the requested item is to be found, splay trees can improve future access to the same or similar items when the input exhibits temporal or spatial locality (see Fig.~\ref{fig:bst_root_3}).  Note that a red-black tree, an AVL tree or any similar self-balancing tree is not self-adjusting, in that it can rearrange only with respect to the items \emph{stored} in it but not with respect to the queries \emph{posed} to it.

\begin{figure}
 \centering
 \begin{tikzpicture}[level distance=30pt,
   level 1/.style={sibling distance=30pt},
   level 2/.style={sibling distance=30pt},
   level 3/.style={sibling distance=30pt}]
   % Left
   \node[circle,draw,minimum size=0.5cm,inner sep=1pt] (3a) {3}
   child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (2a) {2}}
   child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (5a) {5}
     child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (4a) {4}}
     child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (6a) {6}}
   };
   % Right
   \node[circle,draw, minimum size=0.5cm, inner sep=1pt] (4b) at (5.5,0) {4}
   child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (3b) {3}
     child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (2b) {2}}
     child[missing] {}
   }
   child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (5b) {5}
     child[missing] {}
     child {node[circle,draw,minimum size=0.5cm,inner sep=1pt] (6b) {6}}
   };
   % Arrow
   \node (draw=none) at (2.8,-1.5) [font=\large]{$\Rightarrow{Find(4)}\Rightarrow$};
 \end{tikzpicture}
 \caption{Splay-tree with elements 2, 3, 4, 5, 6. After accessing node 4 it is moved to the root that makes a subsequent lookup to it faster, while the tree is kept almost perfectly balanced.}
 \label{fig:bst_root_3}
\end{figure}

\begin{figure*}[t]
  % \begin{tabularx}{\textwidth}{D *{2}{s}}
  \begin{tabular}{m{.3\textwidth} m{.31\textwidth} m{.31\textwidth}}
  % \begin{tabular}{ccc}
    \hspace{28pt}\input{fig/sim-legend.tex}\\
    \multirow{-6.4}{*}{\subcaptionbox{List lookup/uniform input\label{fig:multicore-list-uniform}}{\input{fig/list/uniform-100k/multicore_list.tex}}}%
    & \hspace{18pt}\subcaptionbox{List lookup/Zipf input\label{fig:multicore-list-zipf}}{\input{fig/list/zipf-100k/multicore_list.tex}}
    & \hspace{8pt}\subcaptionbox{List lookup/uniform/single-core\label{fig:singlecore-list-uniform}}{\input{fig/list/uniform-10k/singlecore_list.tex}}
    \\
    & \hspace{18pt}\subcaptionbox{Cache lookup/uniform input\label{fig:multicore-cache-uniform}}{\input{fig/cache/uniform-50k-2/mcore_cache.tex}}
    & \hspace{8pt}\subcaptionbox{Tree lookup/uniform input\label{fig:singlecore-tree-uniform}}{\input{fig/tree/uniform-500/multicore_tree.tex}}
  % \end{tabularx}
  \end{tabular}
  \caption{Static vs. self-adjusting distributed systems scaling laws with round-robin and hash-based load balancing: (a) static vs. MTF list access speedup on uniform input ($m$=100k); (b) static vs. MTF list speedup on skewed input ($m$=100k, Zipf power law with $\alpha=1.01$), (c) static vs. MTF list access goodput with multiple threads running on a \emph{single core} for uniform input ($m$=10k); (d) cache access on uniform input ($m$=50k, $\delta=0.05$, $\rho=100k$ cycles); and (e) static balanced vs. splay tree speedup ($m=500$, $w=100k$ cycles).  Panels (a), (b), (d) and (e) show multicore speedup as the function of the number of CPU cores, each running a single worker, while (c) shows the single-core throughput (goodput) using an increasing number of lightweight parallel threads.}
  \label{fig:dist-self-adjusting-eval}
\end{figure*}

Splay trees are widely used to adaptively speed up associative memory and data compression algorithms \cite{jones1988application}, as well as a building block for more complex self-adjusting algorithms.

% Self-adjusting data structures are widely used in algorithms and computer systems, e.g., in computing point maxima and convex hulls~\cite{BentleyCL93}, organizing lists of identifiers in program compilation and interpretation~\cite{HesterH85}, detecting collisions in hash tables~\cite{HesterH85}, or compressing arbitrary input~\cite{BentleySTW86}. And indeed, every cache management scheme can be viewed as a self-adjusting data structure as well







\subsection{Superlinear scaling (previous)}
\label{sec:arch-scaling}


So how can locality-boosting load balancing and self-adjusting algorithms, when used together in a distributed system, produce superlinear scaling? Below we present an example, \emph{distributed self-adjusting list lookup}, along with a performance analysis as demonstration. Our architecture consists of a locality-boosting partitioning load-balancer (see Fig.~\ref{fig:locality-boosting-lb}) combined with a self-adjusting move-to-front list (see Fig.~\ref{fig:mtf-example}) implemented in the workers. The rationale for why this design achieves superlinear scaling is the following.

Suppose that there are $m$ items to be stored in the list and $k$ workers, each maintaining an independent index into the list. Suppose further that, at the input, requests can be received for any of the $m$ items. To make things more difficult we assume uniform request distribution on the system's input, which is, recall, the worst case for any self-adjusting algorithm by being totally \emph{unpredictable}. Thus, for a single worker move-to-front reordering has no useful effect and the worst case access time is $m$, identical to that of a static linked list.

Now suppose we move from 1 worker to $k$ parallel workers. This results, within our architecture, that the load balancer effectively partitions the uniformly distributed input on $m$ items into $k$ uniformly distributed input streams for only $\frac{m}{k}$ different items (see Fig.~\ref{fig:locality-boosting-lb}). This means that the workers' input features a much higher spatial locality than the system's input (which sports none).  Had we used a random or a round robin load balancer the workers would still see all the $m$ possible inputs, just with a sampled uniform distribution, and no locality. After a while, each MTF list in the workers will have its specific subset of $\frac{m}{k}$ items moved to the first $\frac{m}{k}$ positions (in an arbitrary order), reducing the worst-case lookup time from $m$ (1 worker) to $\frac{m}{k}$ ($k$ workers). This introduces $k\times$ speedup compared to the single-threaded case.

Then, superlinear speedup is merely a product of two simultaneous $k\times$ speedup factors: one $k\times$ speedup comes from the self-adjusting list getting progressively faster as we add new workers (recall the ``scaled size'' model from \S\ref{sec:backgound-dist-cache}), and another $k\times$ speedup because we extend the total compute capacity available to the system $k$ times. The effective speedup is then just the multiple of the two, yielding $k^2$ times speedup in total. Plugging into Amdahl's law we get the \emph{scaling law for distributed MTF lists on uniform input} (see Fig.~\ref{fig:amdahl}):
\begin{equation}\label{eq:mtf-perf}
  S_l(k) = \frac{T_l(1)}{T_l(k)} = \frac1{s + \frac{1-s}{k^2}} \enspace .
\end{equation}

% We used this scaling law as the graphical illustration for superlinear scaling in Fig.~\ref{fig:amdahl}. 
For small values of $k$ we obtain $O(k^2)$ scaling, despite that uniform request distribution is the worst case for self-adjustments. This hints at a great future potential for networking workloads that typically exhibit highly skewed request distributions~\cite{832484}.

%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

