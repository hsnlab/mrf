% !TEX ROOT = ./distributed_mrf.tex
\section{Distributed self-adjusting systems}
\label{sec:architecture}

We now present a general distributed systems architecture, which, as we show theoretically and empirically later, produces faster-than-linear scaling in several disparate problem domains. Our main observation is that, whenever genuinely observed, superlinear scaling assumes two critical components: a policy to dispatch jobs to workers in a way to increase the locality of reference in the per-worker input streams, plus an algorithm that can adaptively exploit the structure in the input to process it more efficiently. Our architecture is purely a software technique in that it does not require the addition of new cache space to a system. Nonetheless, it contains distributed caching as a special case and hence automatically takes advantage of additional fast memory, if available.

\subsection{Locality-boosting load balancing}
\label{sec:lb-lb}

The first crucial component in our architecture is a locality-boosting load balancer.  In this context, load balancing refers to the distribution of computational work or incoming network traffic across multiple parallel \emph{workers} (servers, processors, or nodes). A good load balancing strategy ensures optimal resource utilization, minimizes response time, avoids overloading any single resource, and, as we argue below, improves the locality in the input presented to the workers. 

In the context of this paper, \emph{locality of reference} is the property of a sequence of inputs that subsequent items are statistically dependent on each other. Such structure in the input can then be readily exploited by the proper algorithm \cite{SleatorT85Splay, BentleyCL93, HesterH85, HesterH85, BentleySTW86, Avin0020, ParkM12} or a runtime optimization framework \cite{276946,246322,10.1145/3503222.3507769,procieee_2019} to improve the performance of the code that processes it. A request set with minimal locality is uniformly distributed on the entire input domain and hence unpredictable, while one with maximal locality contains only a single item, i.e., maximally predictable. A \emph{locality-boosting load balancing} policy is then a request dispatching strategy that can statistically or deterministically improve the locality of reference experienced by the worker threads, \emph{turning an unpredictable system input into multiple streams of predictable input} to be processed by the workers (see Fig.~\ref{fig:locality-boosting-lb}).

\begin{figure}
  \centering
  \includegraphics[width=.85\linewidth]{fig/schema.pdf}
  \caption{A locality boosting load balancer partitions the input sequence of a given locality into subsequences with higher locality. Self-adjusting data structures perform better on inputs with higher locality.}
  \label{fig:locality-boosting-lb}
\end{figure}

We distinguish two types of locality in this context. \emph{Spatial locality} means that the distribution of requests on the input domain is statistically biased towards a particular subset of the items. One way to ensure this in the load balancer is to \emph{partition} the input domain into disjoint subsets, so that worker's input distributions are concentrated on a smaller set of inputs. For instance, the hash-based load balancer we used previously to show superlinear scaling with distributed caching is such a partitioning load balancer. In contrast, a round robin or a uniform random load balancer will export its own spatial input locality unchanged to the workers. A related concept is \emph{temporal locality}, which refers to the reuse of specific items in the input within a relatively small time duration. One way to boost temporal locality is to reorder items within a time window: e.g., Reframer applies controlled delays to particular packets in a packet batch to boost temporal locality and, thereby, enable more efficient processing \cite{276946,246322}.

\subsection{Self-adjusting algorithms}
\label{sec:sa-alg}

The second critical enabler for superlinear scaling is \emph{self-adjusting algorithms}. Self-adjustment is a general term referring to the property of a dynamic data structure to \emph{automatically reorganize itself based on the sequence of inputs it receives}, in order to optimize performance for future operations on frequently accessed items. Internal data reorganization always introduces extra complexity and overhead compared to a static data structure. Thus, self-adjustment can improve performance only if the input processed by the algorithm exhibits a certain amount of spatial or temporal locality.

Next we review the most prominent self-adjusting data structures from the literature (but see also \cite{BoseDL08, Avin0020, ParkM12}).

% For further examples, see self-adjusting skip lists~\cite{BoseDL08}, push-down trees~\cite{Avin0020}, or geometric data storages \cite{ParkM12}.

\noindent%
\textbf{Caches.} %
As the simplest but most universal self-adjusting data structure, caches can serve frequently accessed items fast by storing them in a  software or a hardware fast memory. This is typically much faster than if we had to run the request through the full processing pipeline or the slow backing store. Caches have that almost magical capability of self-adaptation, without us having to engineer any prior knowledge of the input into the cache mechanism apart from a promise that it has nontrivial locality. When the promise is true, caches are an inexpensive way to improve throughput and response time. When there is no locality in the input, however, caches usually just add extra latency and overhead.
%
Note that caches do not necessarily have to come in the form of hardware memory: a fast key-value store is a candidate cache for a slow database \cite{10.5555/1012889.1012894}, a kernel fast-path flow cache is a useful way to speed up a slow user-space software switch \cite{188960}, etc.

% However, caches also come with additional complexity and overhead: cache entries have to be created for storing recently accessed data, invalidated when the backing data changes, evicted when the cache is full, and synchronized to consistently represent data that may be present in multiple caches. % Hardware implementations are appealing from this aspect by hiding the extra complexity behind a fast on-chip implementation.

% The performance of a cache is determined by the cache hit rate $\delta$, defined as the ratio of operations served from fast memory to those served on the slow path, and $\rho$, the penalty of a cache miss. In general, for a common LFU or LRU cache the higher the locality in the input and the bigger the cache compared to the input domain, the higher the cache hit rate and the lower the response time. When there is no locality in the input, caches usually just add extra latency and overhead.

\noindent%
\textbf{List lookup.} %
One of the most widely used self-adjusting data structures is the \emph{move-to-front list}. Suppose we wish to store a list of $m$ items in a way so that reordering, insertion and deletion are fast, while lookup is also reasonably efficient. A straightforward choice is a static linked list. Here, the cost of accessing an item at position $i$ is exactly $i$. Then, any linked list can easily be upgraded to a self-adjusting list using the move-to-front (MTF) heuristics: after accessing an item it is moved to the front, which improves lookup time for future requests of the same item at minimal cost (see Fig.~\ref{fig:mtf-example}). The MTF heuristics comes with appealing theoretical properties, namely that blindly moving the accessed item to the front of the list is close to the best reordering strategy one could design, even if one knew all future requests \cite{SleatorT85}. MTF lists handle both spatial and temporal locality. For uniformly distributed input MTF lists usually add nontrivial overhead compared to static lists due to the frequent and useless relinking of the list.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{fig/mtf.pdf}
  \caption{A self-adjusting list containing nodes A, B, C and D serves the request to C and moves C to the front of the list to speed up future accesses to C.}
  \label{fig:mtf-example}
\end{figure}

Classic applications of MTF lists are information retrieval systems, compression~\cite{BentleySTW86}, etc. In general, any use case is a potential candidate application for MTF where the task is to match a request against a list of complex rules that do not lend themselves readily to be arranged into a fast lookup structure (e.g., a search tree), like inference in explainable rule-based AI % and expert systems
\cite{dovsilovic2018explainable}, rule matching in OpenFlow and P4 reference software switches \cite{openflow}, packet classification in networking (see later), etc.  We note that caching is a subset of list lookup, in that every algorithm for list reorganization gives rise to a different cache management algorithm \cite{SleatorT85}.  

\noindent%
\textbf{Search trees.} %
A search tree is an efficient tree data structure for locating specific keys from within an ordered set. A \emph{splay tree} is a self-adjusting version of a static search tree, in that it can dynamically reorganize itself by moving popular items closer to the root of tree and less frequently accessed elements to the bottom, while keeping the tree relatively well-balanced \cite{SleatorT85Splay, BoseDL08, Avin0020}. Since access time in a search tree is determined by the depth at which the requested item is to be found, splay trees can improve future access to the same or similar items when the input exhibits temporal or spatial locality (see Fig.~\ref{fig:bst_root_3}).  Note that a red-black tree, an AVL tree or any similar self-balancing tree is not self-adjusting, in that it can rearrange only with respect to the items \emph{stored} in it but not with respect to the queries \emph{posed} to it.

\begin{figure}
 \centering
 \begin{tikzpicture}[level distance=25pt,
   level 1/.style={sibling distance=25pt},
   level 2/.style={sibling distance=25pt},
   level 3/.style={sibling distance=25pt}]
   % Left
   \node[circle,draw,minimum size=0.4cm,inner sep=1pt] (3a) {3}
   child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (2a) {2}}
   child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (5a) {5}
     child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (4a) {4}}
     child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (6a) {6}}
   };
   % Right
   \node[circle,draw, minimum size=0.4cm, inner sep=1pt] (4b) at (5.5,0) {4}
   child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (3b) {3}
     child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (2b) {2}}
     child[missing] {}
   }
   child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (5b) {5}
     child[missing] {}
     child {node[circle,draw,minimum size=0.4cm,inner sep=1pt] (6b) {6}}
   };
   % Arrow
   \node (draw=none) at (2.8,-1) [font=\normalsize]{$\Rightarrow{Find(4)}\Rightarrow$};
 \end{tikzpicture}
 \caption{Splay-tree with elements 2, 3, 4, 5, 6. After accessing node 4 it is moved to the root that makes a subsequent lookup to the same node faster, while the tree is well-balanced.}
 \label{fig:bst_root_3}
\end{figure}

Splay trees are widely used to adaptively speed up associative memory and data compression algorithms \cite{jones1988application}, as well as a building block for more complex self-adjusting algorithms.

% Self-adjusting data structures are widely used in algorithms and computer systems, e.g., in computing point maxima and convex hulls~\cite{BentleyCL93}, organizing lists of identifiers in program compilation and interpretation~\cite{HesterH85}, detecting collisions in hash tables~\cite{HesterH85}, or compressing arbitrary input~\cite{BentleySTW86}. And indeed, every cache management scheme can be viewed as a self-adjusting data structure as well

\subsection{Superlinear scaling}
\label{sec:arch-scaling}

So how can locality-boosting load balancing and self-adjusting algorithms, when used together in a distributed system, produce superlinear scaling? First, we present a demonstration on a particular instantiation of the architecture, \emph{distributed self-adjusting list lookup}, and then we provide a formal scaling characterization for general distributed self-adjusting systems.

Consider a partitioning load-balancer (see Fig.~\ref{fig:locality-boosting-lb}) combined with a self-adjusting move-to-front list (see Fig.~\ref{fig:mtf-example}) implemented in the workers. Suppose that there are $m$ items to be stored in the list and $k$ workers, each maintaining an independent index into the list. To make things more complicated, we assume uniform request distribution on the entire input domain $m$ at the system's input, which is, recall, the worst case for any self-adjusting algorithm by being totally \emph{unpredictable}. Thus, for a single worker move-to-front reordering has no useful effect and the worst case access time is $m$, identical to that of a static linked list.

Now suppose we move from 1 worker to $k$ parallel workers where $k \le m$. This results, within our architecture, that the load balancer effectively partitions the uniformly distributed input on $m$ items into $k$ uniformly distributed input streams for only $\sfrac{m}{k}$ different items (see Fig.~\ref{fig:locality-boosting-lb}). This means that the workers' input features a higher spatial locality than the system's input (which sports none).  Had we used a random or a round robin load balancer the workers would still see all the $m$ possible inputs, just with a sampled uniform distribution, and no locality. After a while, each MTF list in the workers will have its specific subset of $\sfrac{m}{k}$ items moved to the first $\sfrac{m}{k}$ positions (in an arbitrary order), reducing the worst-case lookup time from $m$ (1 worker) to $\sfrac{m}{k}$ ($k$ workers). This introduces $k\times$ speedup compared to the single-threaded case.

Then, superlinear speedup is merely a product of two simultaneous $k\times$ speedup factors: one $k\times$ factor comes from the self-adjusting list getting progressively faster as we add new workers (recall the ``scaled size'' model from \S\ref{sec:backgound-dist-cache}), and another $k\times$ speedup because we scale the total compute capacity available to the system $k$ times. The effective speedup is then just the multiple of the two, yielding $k^2$ times speedup in total. Plugging into Amdahl's law we get the \emph{scaling law for distributed MTF lists on uniform input} (see Fig.~\ref{fig:amdahl}):
\begin{equation}\label{eq:mtf-perf}
  S_l(k) = \frac{T_l(1)}{T_l(k)} = \frac1{s + \frac{1-s}{k^2}} \qquad k \le m \enspace .
\end{equation}

% We used this scaling law as the graphical illustration for superlinear scaling in Fig.~\ref{fig:amdahl}. 
For small values of $k$ we obtain $O(k^2)$ scaling. As $k$ grows sufficiently large, say, when $k=m$, the workers' input reduces to a singleton ($\sfrac{m}{k}=1$). From this point the distributed MTF list reduces into a simple parallel hash table and superlinear speedup degrades into an ``ordinary'' Amdahl's scaling profile until speedup eventually blocks on a serial bottleneck (e.g., the sequential load balancer).  For anything between, the system adaptively finds the best combination of an MTF list and a hash-table, producing a quadratic scaling.

% The next theorem generalizes the superlinear scaling characterization from  MTF lists and simple uniform requests to an arbitrary distributed self-adjusting system processing an arbitrary input stream.
% \begin{theorem}\label{thm:general-scaling-law}
%   Consider a load balancer that dispatches inputs $\sigma$ taken from a universe $\mathcal{U}$ to $k$ identical parallel workers $W_1, W_2, \ldots, W_k$, each running an instance of a self-adjusting algorithm $D$, using a deterministic function $f_k : \mathcal{U} \to \{1, 2, \ldots, k\}$ that partitions the input universe $\mathcal{U}$ into $k$ disjoint subsets $\mathcal{U}_1, \mathcal{U}_2, \ldots, \mathcal{U}_k$. We call an algorithm $D$ ``self-adjusting'' if the running time of $D$ is a submodular function of its input universe $\mathcal{U}$, that is, for any pair of input sequences $\sigma_1 \subseteq \mathcal{U}_1$ and $\sigma_2 \subseteq \mathcal{U}_2 \subset \mathcal{U}_1$ it holds that $T_D(\sigma_2) < T_D(\sigma_1)$, where $T_D$ is the running time of $D$ on input $\sigma$. Consider the speedup in the form
%   \begin{displaymath}
%     S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{k \cdot g(k)}} \enspace ,
%   \end{displaymath}
%   where $g(k)$ is a function characterizing superlinear speedup.  Then, there is a threshold $K$ so that:
% \begin{description}\setlength\itemsep{0pt}%
% \item[Superlinear regime:] if $k < K$ then $g(k) > 1$ and $\frac{d S(k)}{d k} > 1$.
% \item[Sublinear regime:] if $k\ge K$ then $g(k) = 1$ and $\frac{d S(k)}{d k} \le 1$.
% \end{description}
% \end{theorem}

% See the proof in the Appendix.

% This result warrants that superlinear growth in distributed self-adjusting systems is \emph{universal}, in that it should appear for \emph{any} combination of a partitioning load balancer and a self-adjusting algorithm and on \emph{any} input distribution, but it is only a \emph{transient} phenomenon: at a certain threshold $K$ the system goes through a \emph{phase transition}, where the initial superlinear growth reduces to an ``ordinary'' Amdahl's profile.  For distributed caching phase transition occurs at $K=\sfrac{m}{c}$, when total cache capacity $k c$ is enough to store the entire request set (recall the analysis in \eqref{eq:dist-cache} and Fig.~\ref{fig:dcache-analysis}), and for distributed MTF lists $K=m$, i.e., phase transition occurs when workers' input reduces to a singleton set.


\begin{figure*}[t]
  % \begin{tabularx}{\textwidth}{D *{2}{s}}
    % \hspace{28pt}
    \centering
    \input{fig/sim-legend.tex}
    % \\
  \renewcommand{\arraystretch}{0.01}%
  \begin{tabular}{m{.3\textwidth} m{.31\textwidth} m{.31\textwidth}}%
  % \begin{tabular}{ccc}
  % \\
    \multirow{-505}{*}{\subcaptionbox{List lookup/uniform input\label{fig:multicore-list-uniform}}{\input{fig/list/uniform-100k/multicore_list.tex}}}%
    & \hspace{18pt}\subcaptionbox{List lookup/Zipf input\label{fig:multicore-list-zipf}}{\input{fig/list/zipf-100k/multicore_list.tex}}
    & \hspace{8pt}\subcaptionbox{List lookup/uniform/single-core\label{fig:singlecore-list-uniform}}{\input{fig/list/uniform-10k/singlecore_list.tex}}
    \\
    & \hspace{18pt}\subcaptionbox{Cache lookup/uniform input\label{fig:multicore-cache-uniform}}{\input{fig/cache/uniform-50k-2/mcore_cache.tex}}
    & \hspace{8pt}\subcaptionbox{Tree lookup/uniform input\label{fig:singlecore-tree-uniform}}{\input{fig/tree/uniform-500/multicore_tree.tex}}
  % \end{tabularx}
  \end{tabular}
  \caption{Static vs. self-adjusting distributed systems scaling laws with round-robin and hash-based load balancing: (a) static vs. MTF list access speedup on uniform input ($m$=100k); (b) static vs. MTF list speedup on skewed input ($m$=100k, Zipf power law with $\alpha=1.01$), (c) static vs. MTF list access goodput with multiple threads running on a \emph{single core} for uniform input ($m$=10k); (d) cache access on uniform input ($m$=50k, $\delta=0.05$, $\rho=100k$ cycles); and (e) static balanced vs. splay tree speedup ($m=500$, $w=100k$ cycles).  Panels (a), (b), (d) and (e) show multicore speedup as the function of the number of CPU cores, each running a single worker, while (c) shows the single-core throughput (goodput) using an increasing number of lightweight parallel threads.}
  \label{fig:dist-self-adjusting-eval}
\end{figure*}

In general, superlinear speedup emerges as the superposition of two related speedup factors. First, by splitting the input into multiple input streams of improved locality, the locality-boosting load balancer reduces the ``effective size'' of the workload the workers will have to process. Denote by $\ell(k)$ the ``workload reduction'' attainable on the given input this way with $k$ workers. Second. there is a ``parallelizability'' gain, denote this by $q(k)$, that is obtained by the self-adjusting workers processing the reduced workloads.  In the Appendix we present a formal definition of these terms and prove the below general \emph{scaling law for distributed self-adjusting systems}:
\begin{displaymath}
    S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{q(k) \cdot \ell(k)}} \enspace .
\end{displaymath}

Our formal characterization describes the cases when superlinear scaling is attainable (positive results) and when it is not (negative results). In general, whenever superlinear scaling is attained the speedup profile always comprises to scaling regimes. In an initial \emph{superlinear scaling regime} we have $q(k) \cdot \ell(k) > k$, with both the effective size reduction and parallelizability gain contributing to faster-than-linear speedup. Eventually, superlinear growth peters out and the system falls back to a \emph{sublinear scaling regime} with $q(k) \cdot \ell(k) \le k$.

\subsection{Simulations}
\label{sec:sims}

Next, we present a series of simulation studies to confirm that locality-boosting load balancing combined with self-adjusting workers (but only this combination) yields faster-than-linear scaling over a broad selection of load balancing policies, self-adjusting algorithms, and input distributions.

Our simulator was written in Go, using lightweight threads (goroutines) managed by the Go runtime to run a given number of workers in parallel, a home-grown implementation of static and MTF lists, and standard Go modules for LRU caches \cite{golang-lru}, static balanced trees \cite{golang-btree} and splay trees \cite{golang-splay}. In order to make tree lookup CPU bounded we used an ``expensive'' order underneath the tree, where every comparison operation costs a configurable $w$ number of extra cycles. The simulator creates the specified combination of a load balancer, $k$ worker threads running the selected lookup algorithm, and a random input sequence with a given request distribution, and then performs a configurable number of lookup operations and measures the total execution time with nanosecond precision. To obtain a full picture, the total execution time includes the transient time needed to warm up the self-adjusting algorithms as well as the overhead of request generation, goroutine scheduling, and memory management. For the specification of the evaluation platform, refer to \S\ref{sec:sa-nf-tables-eval}.

Fig.~\ref{fig:dist-self-adjusting-eval} shows the results. First, the immediate observation is that \emph{the right combination of a locality-boosting load balancer and a self-adjusting algorithm robustly delivers superlinear speedup}, irrespectively of the problem domain or the input distribution. Even for a worst-case uniform input we obtain $3,300\times$ speedup for list access on 48 CPU cores, almost $70\times$ of ``ideal'' linear speedup, $200\times$ speedup on LRU caches and $65\times$ speedup on tree search with 36 CPU cores. In most cases, the superlinear growth is so dominant that we can hardly put Amdahl's scaling on the same diagram.  Second, \emph{only the combination of locality-boosting load balancing and self-adjusting algorithms produces superlinear speedup}, all other combinations (i.e., round robin with any algorithm or static algorithm with any load balancer) fall back to Amdahl's scaling.  Third, \emph{self-adjustment clearly has its overhead}. This can be observed in Fig.~\ref{fig:singlecore-list-uniform} that shows the absolute throughput instead of the relative speedup. Here, the single-threaded self-adjusting algorithm is slower than the static one due to processing an unpredictable input. Fourth, \emph{the overhead of self-adjustment is irrelevant for more than one worker, or with skewed request distributions}. For instance, on a Zipf input distribution (Fig.~\ref{fig:multicore-list-zipf}) even the single-threaded self-adjusting version is already $2$--$2.5\times$ faster in an absolute term (not shown in the figure). However, \emph{only} combined with a locality-boosting load balancer it produces superlinear speedup.

Finally, we show that our architecture can yield a linear parallel performance gain even if we do not even add additional CPU to the system. Fig.~\ref{fig:singlecore-list-uniform} shows an evaluation that was executed with an \emph{increasing number of parallel threads sharing a single CPU core}, with a little surplus CPU for the load balancer.  The results indicate that the parallel self-adjusting system (but \emph{only} this combination!) delivers linear speedup.  How can parallelization benefit performance when the total CPU power available to the system is kept constant? Recall, in the multicore case superlinear speedup emerges due to the superposition of two independent $k\times$ speedup trends, one delivered by self-adjustment and another by the $k\times$ scaling of the total CPU power. When the total available CPU is limited only first $k\times$ speedup factor is in effect, resulting in the observed linear scaling trend.

It is important to stress that superlinear speedup is only possible with respect to a single-threaded single-core baseline. Had we normalized with respect to a multi-threaded baseline constrained to a single core (as in Fig.~\ref{fig:singlecore-list-uniform}) we would see only a linear speedup. See the Appendix for a precise formal characterization of the conditions under which superlinear speedup can be attained.

% glitches are cpu architecture specific

% LET's SKIP THIS: highly speculative!!!!!!!!!!!1
%
% \subsection{Revised Amdahl's law}
% \label{sec:sims}

% An interpretation of superlinear scaling: if we introduce the notion of the ``virtual job size''. Implicit in Amdahl's law \eqref{eq:amdahl} is that the job size remains the same independently of $k$. Parallel self-adjustments, however, may actually \emph{decrease} the amount of work each worker has to perform per each request. Let $b(k)$ denote the ``virtual job size'' perceived by each worker when the number of  workers is $k$. We observe that in parallel self-adjusting systems $b(k)$ is decreasing in $k$; e.g., for MTF we have $b(k) = \frac1{k}$.

% \begin{equation}\label{eq:revised-amdahl}
% S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{k^{\alpha}}} \enspace .
% \end{equation}

% Amdah's law for $\alpha=1$, distributed MTF scaling for $\alpha=1$, superlinear scaling with $\alpha>1$

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

