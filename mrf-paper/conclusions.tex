\section{Conclusions}\label{sec:conclusions}

we advocate a new style for architecting distributed systems: LB-LB+SA-algs, yields superlinear scaleup

adding a minimal caching can already imply faster-than-linear scaling: linux fib? 

can we reproduce our findings with other SA algorithms?
\begin{itemize}
\item any linear rule matching use cases immediately qualify: DPI + explainable/rule-based AI
\item \emph{any} combination of a self-adjusting algorithm used in workers, with a sufficient LB, will yield superlinear speedup
\item SA distributed AI/ML training????
\item map/reduce
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

