\appendix





\section{Superlinear Scaling Theorem}
\label{sec:apx}

Our law informally stated the following: no self-adjusting system can scale better than
\begin{equation*}
  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s}{k \cdot \ell}} \enspace ,
\end{equation*}
for some value $\ell$ that depends on how well the load balancer $f_k$ reduces the input $\sigma$.

Before we state the theorem formally, we need to provide the formal definition of $\ell$.
The value of $\ell$ depends on the input $\sigma$, data structure $D$ and the load balancer $f_k$. Hence, $\ell$ indirectly relies on $k$ through $f_k$ (these are tied together in our architecture). Furthermore, for technical reasons, $\ell$ depends on the serial portion of the workload $s$.
To define $\ell$, we need to relate the input $\sigma$ with the load balancer $f_k$ and the data structure $D$.
We begin by defining the data structure $D$.



\paragraph*{Self-adjusting data structures and working sets.}
Our law captures various self-adjusting data structures, such as lists, splay trees or caches.
In these data structures, the cost of accessing an item depends on the internal structure and changes over time depending on the history of requests.
Self-adjusting data structures have a property that the cost of accessing an item $x$ at time $t$ depends on the number of distinct items requested since the last access of $x$.
This is often referred to as the \emph{working set property}, and it can hold in an amortized or probabilistic sense.

We assume that for $D$, there exists a function $g_D$ that captures this cost
\[
	\textsf{cost}(x, t, \sigma) = g(|W_t(x)|) \enspace ,
\]
where $W_t(x)$ is the number of distinct requests to items other than $x$ since the last request to $x$.
% The cost of rearrangement after access is usually proportional to access cost.
To illustrate $g_D$, the simplest example is a self-adjusting list running Move-To-Front algorithm, which has the cost of accessing an item is linear, $g_\textsf{list}(x,t,\sigma) = |W_t(x)| + 1$. Splay trees have a logarithmic access cost, $g_\textsf{splay}(x,t,\sigma) = \log |W_t(x)| + 1$. LRU caches are similar to Move-to-Front list, but the cost model for $g_\textsf{LRU}$ is non-linear: for a cache of size $B$, the cost is 1 if the working set is larger than $B$, and 0 otherwise (see~\cite{SleatorT85} for the relationship between caching algorithms and self-adjusting list algorithms).

For our upper bound on scaling to hold, all we assume is that $g_D$ is a monotonic function of $|W_t(x)|$.

\paragraph*{Load balancer isolates working sets.}
Recall that the load balancer partitions the stream into $k$ streams $\sigma(M_1), \sigma(M_2), \ldots, \sigma(M_k)$ and dispatches them to the machines.
These streams may have reduced working set sizes at the machines in comparison to the working set sizes of the original stream $\sigma$.
Precisely, a load balancer $f_k : \mathcal{U} \to \{1, 2, \ldots, k\}$ partitions the universe $\mathcal{U}$ into $k$ subsets $\mathcal{U}_1, \mathcal{U}_2, \ldots, \mathcal{U}_k$, and the working set for machine $i$ at the time $x = \sigma_t$ is requested is $W_t(x, t, \sigma(M_i)) = W_t(x, t, \sigma) \cap \mathcal{U}_i$.

\paragraph*{Defining $\ell$: from an additive to a multiplicative law.}
In total, we observe cost savings from load balancing for the stream $\sigma$, data structure $D$ characterized by a function $g$, and a load balancer $f_k$. We denote them as
\[
	r = \sum_{t} g(|W_t(x)|) - \sum_{t} g(|W_t(x)| \cap \mathcal{U}_i) \enspace .
\]
Then, the total work decreases (a strict deviation from Amdahl's law) by $r$ for the stream $\sigma$. Let $r'$ be a normalized value of $r$ with respect to the total non-serial work of the system. Then, applying Amdahl's law to the reduced workload, we obtain the following theorem.
\begin{equation*}
  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s - r'}{k}} \enspace ,
\end{equation*}
By setting $\ell = \frac{(1-s)\cdot k}{1-s-r'}$, we obtain the following theorem.

\begin{theorem}
	\label{thm:superlinear}
	Fix an input sequence $\sigma$.
	Assume that a self-adjusting data structure $D$ whose cost function depends on its working set size with a cost function $g_D$ that is monotonically increasing. 
	Then, for any number of machines $k$, the scaling of the system is upper-bounded by
	\begin{equation*}
	  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s}{k \cdot \ell}} \enspace ,
	\end{equation*}
\end{theorem}
The proof of this theorem is a direct consequence of the Amdahl's law to the reduced workload. The total workload is reduced additively by $r$ (the exact value depends on $g_D$ and $f_k$), which gives the multiplicative law for our choice of $\ell$.


\paragraph*{Consequences.}
The above law has consequences for the scaling of self-adjusting systems. We express these consequences in terms of the additive $r$, but the consequences for $\ell$ are immediate.



\begin{observation}
	\label{obs:list-k2}
	The value of $r$ can be $O(k^2)$ for lists.
\end{observation}
This can happen for $\sigma$ that requests all items of the universe in a round-robin fashion and for $f_k$ that isolates all affinity domains into singletons. 


\begin{observation}
	\label{obs:initial}
	The value of $r$ cannot grow indefinitely with~$k$.
\end{observation}
After $k = |\mathcal{U}|$, all affinity domains are singletons, and increasing $k$ further does not reduce the cost any further, since there are no more working sets to isolate.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:
