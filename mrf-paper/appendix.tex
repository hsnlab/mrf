\appendix





\section{Superlinear Scaling Theorem}
\label{sec:apx}

Our law informally stated the following impossibility result: no self-adjusting system can scale better than
\begin{equation*}
  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s}{k \cdot \ell}} \enspace ,
\end{equation*}
for some value $\ell$ that depends on how well the load balancer $f_k$ reduces the input $\sigma$.

Before we state the theorem formally, we need to provide the formal definition of $\ell$.
The value of $\ell$ depends on the input $\sigma$, data structure $D$ and the load balancer $f_k$. Hence, $\ell$ indirectly relies on $k$ through $f_k$ (these are tied together in our architecture). Furthermore, for technical reasons, $\ell$ depends on the serial portion of the workload $s$.
To define $\ell$, we need to relate the input $\sigma$ with the load balancer $f_k$ and the data structure $D$.
We begin by defining the data structure $D$.



\paragraph*{Self-adjusting data structures and working sets.}
Our law captures various self-adjusting data structures, such as lists, caches and their generalizations.
In these data structures, the cost of accessing an item depends on the internal structure and changes over time depending on the history of requests.
Self-adjusting data structures have a property that the cost of accessing an item $x$ at time $t$ depends on the number of distinct items requested since the last access of $x$.
This is often referred to as the \emph{working set property}, and it can hold in an amortized sense.

With each data structure $D$, there exists an associated cost function $g_D$ 
\[
	\textsf{cost}(x, t, \sigma) = g(|W_t(x)|) \enspace ,
\]
where $W_t(x)$ is the number of distinct requests to items other than $x$ since the last request to $x$.
The working set property is usually an upper bound, but we state the law for the case where the cost is \emph{exactly} the working set size.
With this simplifying assumption, we still capture LRU caches and Move-to-Front lists, but not splay trees\footnote{todo: splay tree is only an upper bound, and holds in an amortized sense}. 
% The cost of rearrangement after access is usually proportional to access cost.

We illustrate $g_D$ for Move-to-Front and LRU. In Move-to-Front the cost of accessing an item is linear: $g_\textsf{MTF}(x,t,\sigma) = |W_t(x)| + 1$.  LRU is the algorithm Move-to-Front casted into the cost model of caching\footnote{See~\cite{SleatorT85} for the relationship between caching algorithms and self-adjusting list algorithms.}. Precisely, the cost model for $g_\textsf{LRU}$ is non-linear: for a cache of size $B$, the cost is 1 if the working set is larger than $B$, and 0 otherwise. Both cost functions $g_\textsf{MTF}$ and $g_\textsf{LRU}$ are monotonically increasing in the working set size. In fact, monotonicity of $g_D$ is a sufficient condition for our theorem to hold.

\paragraph*{Load balancer isolates working sets.}
Recall that the load balancer partitions the stream into $k$ streams $\sigma(M_1), \sigma(M_2), \ldots, \sigma(M_k)$ and dispatches them to the machines.
These streams may have reduced working set sizes at the machines in comparison to the working set sizes of the original stream $\sigma$.
Precisely, a load balancer $f_k : \mathcal{U} \to \{1, 2, \ldots, k\}$ partitions the universe $\mathcal{U}$ into $k$ subsets $\mathcal{U}_1, \mathcal{U}_2, \ldots, \mathcal{U}_k$, and the working set for machine $i$ at the time $x = \sigma_t$ is requested is $W_t(x, t, \sigma(M_i)) = W_t(x, t, \sigma) \cap \mathcal{U}_i$.

\paragraph*{Visualize the cost savings with a complete weighted graph.}
TODO

\paragraph*{Defining $\ell$: from an additive to a multiplicative law.}
In total, we observe cost savings from load balancing for the stream $\sigma$, data structure $D$ characterized by a function $g$, and a load balancer $f_k$. We denote them as
\[
	r = \sum_{t} g(|W_t(x)|) - \sum_{t} g(|W_t(x)| \cap \mathcal{U}_i) \enspace .
\]
Then, the total work decreases (a strict deviation from Amdahl's law) by $r$ for the stream $\sigma$. Let $r'$ be a normalized value of $r$ with respect to the total non-serial work of the system. Then, applying Amdahl's law to the reduced workload, we obtain the following theorem.


\begin{equation*}
  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s - r'}{k}} \enspace ,
\end{equation*}
By setting $\ell = \frac{(1-s)\cdot k}{1-s-r'}$, we obtain the following theorem.

\begin{theorem}
	\label{thm:superlinear}
	Fix an input sequence $\sigma$.
	Assume that a self-adjusting data structure $D$ whose cost function depends on its working set size with a cost function $g_D$ that is monotonically increasing. 
	Then, for any number of machines $k$, the scaling of the system is upper-bounded by
	\begin{equation*}
	  S(k) = \frac{T(1)}{T(k)} = \frac1{s + \frac{1-s}{k \cdot \ell}} \enspace ,
	\end{equation*}
\end{theorem}
The proof of this theorem is a direct consequence of the Amdahl's law to the reduced workload. The total workload is reduced additively by $r$ (the exact value depends on $g_D$ and $f_k$), which gives the multiplicative law for our choice of $\ell$.


\paragraph*{Consequences.}
The above law has consequences for the scaling of self-adjusting systems. We express these consequences in terms of the additive $r$, but the consequences for $\ell$ are immediate.

The value of $r$ cannot grow indefinitely with~$k$.
We observe that after $k = |\mathcal{U}|$, all affinity domains are singletons, and increasing $k$ further does not reduce the cost any further, since there are no more working sets to isolate.
\begin{equation*}\label{eq:mtf-perf}
 \forall \sigma \exists k \frac{T(1)}{T(k)} \neq \frac1{s + \frac{1-s}{k^2}} \enspace ,
 TODO
\end{equation*}


\paragraph*{Strong guarantees for list algorithms.}
Consider a generalized architecture, where we can change the data structure $D$ on each machine $M_i$ to any other data structure $D_i$, even to the offline optimum for the given sequence. 
Then, the analysis of Albers shows that the system cannot improve more than a constant factor above the Move-to-Front algorithm (there exist competitive bounds).


\section{Initial Superlinear Scaling}

Finally, a positive result.
For some inputs, distributed self-adjusting list lookup can achieve \emph{initial} superlinear scaling.

\begin{observation}
	\label{obs:list-k2}
	In the distributed self-adjusting lookup, 
	for infinitely many inputs $\sigma$, there exists  and a constant $k_0$ such that all $k \le k_0$ there exists a load balancer $f_k$ that achieves have locality boosting savings $r' = \Omega(k^2)$.
\end{observation}

\begin{proof}
	Consider an input $\sigma$ that requests selected $k$ items of the universe in a round-robin fashion and repeats an arbitrarily large number of times $m$.
	The baseline $T(1)$ finishes the schedule in time $T(1) = 1$.

Consider $k$ machines. We construct a load balancer $f_k$.
First, we construct an auxiliary graph $G$ of $k$ vertices, where each vertex represents a machine. The graph is weighted, and each edge $(M_i, M_j)$ has the cost that is saved by isolating affinity domains between these two machines. For the special case of $\sigma$, this graph is a complete graph with equal weights.
Consider an ideal load balancer $f_k$ that isolates the affinity domains of all items from $\sigma$ by an arbitrary $k$-cut (components of size $k$)\cite{Frieze97, Mahajan95}. The total savings per each machine amount to $m\cdot(k-1)$.
Then, the distributed system pays $1$ for handling each request in the list lookup model, as opposed to $\Omega(k)$ of the baseline (all items are in the working set, and the function $g_D$ is linear).


TODO. explicitly show scaling by definition



% The distributed system with $k$ machines finishes its perfectly divisible parallel schedule (equal costs of all requests) in time $T(k) = s + W/k$, where $W$ is the total parallel work.


% Then, the total parallel work is 
% 	\[
% 		T(1) / T(k) \ge 1 / (s + (1-s-Omega(k^2))/k)  \enspace .
% 	\]


% The value $k_0$ must be chosen such that the load balancer has enough affinity domains. A trivial value of $k_0$ is $|\mathcal{U}|$, but for inputs with high locality $k_0$ can be smaller. 
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:
