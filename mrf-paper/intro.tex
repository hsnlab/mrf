% !TEX ROOT = ./distributed_mrf.tex
\section{Introduction}\label{sec:introduction}

With the end of Moore's law, computing power in modern systems increasingly comes in the form of parallel processing resources.  A major obstacle faced by network engineers is how to harness this increasingly parallel computing power for scaling distributed systems \cite{265065, 10.5555/3307441.3307467, 10.1145/2815400.2815423, 10.1145/3098822.3098826, 10.5555/3154630.3154639}.

In horizontally scaled applications a load balancer dispatches jobs across a fleet of workers that process the jobs in parallel \cite{10.5555/3235491}.  In the context of \emph{web applications}, HTTP load balancers \cite{194966, 211279, 9552525} distribute requests across a swarm of backend web servers.  % by hashing over the source IP address (``sticky sessions'').
% This way, requests from the same client will hit the same backend server, improving request locality.
% Meanwhile, resource state is maintained in a key-value store or a relational database.
Multicore \emph{OS network stacks} \cite{211263, 10.1145/3359989.3365412, 10.1145/3452296.3472914} run multiple instances of the networking logic on different CPUs and leverage the NIC to dispatch packets to CPU cores. % In order to avoid packet reordering and improve CPU cache performance, the NIC typically computes a hash over the packet header fields to select the CPU (RSS, RPS, etc.).
In \emph{sharded key-value stores} \cite{ghigoff2021bmc} different servers handle different portions of the key-space.  The system's overall goal is to achieve the greatest possible parallel speedup with a given number of workers, in order to minimize the execution time of a single task or maximize the number of completed tasks in a given time period.

Suppose a web app handles 100 requests per second using a single server. As we add another server we expect a throughput to increase to 200 requests per second, or slightly less if the system is not perfectly parallel \cite{10.1145/1465482.1465560}. (Sub)linear scaling feels almost evident in this context: as the additional capacity can be consumed at 100 percent efficiency at best, we can ``get at most equal bang for the capacity buck'' \cite{10.1145/2773212.2789974}. Strikingly, faster-than-linear (or \emph{superlinear}) growth seemingly defying this common wisdom has been observed experimentally in many high-performance computing applications and distributed systems \cite{scalability-analyzed, 10.5555/1012889.1012894, 271208, icsoft20, sdn-analytitcs, 556383, 6483679, 10.1007/978-3-319-77610-1, dobb-1, dobb-2, 10.1145/3627703.3629574, 7733347, 80148}.  Superlinear growth feels particularly alluring in this context, in that it assumes a system that somehow manages to produce more work than the computer capacity available to it \cite{10.1145/2773212.2789974}.

In fact, faster-than-linear scaling is not supernatural at all. Suppose our sample web app serves a set of static assets (web pages, images, etc.) and assume each server is assigned a fixed subset of the assets, with a load balancer carefully routing client requests for each asset to the proper server. As the number of servers increases each server perceives requests to a progressively smaller subset of the assets, which may allow it to finish servicing requests faster, say, by \emph{caching} the most popular assets in fast memory \cite{10.5555/1012889.1012894, 271208}. The combined speedup resulting from the increase of web server capacity and the decrease of servers' ``virtual job size'' due to improved cache efficiency often yields superlinear scaling \cite{556383, dobb-1, dobb-2, 7733347, 80148, wikipedia, 10.1145/42411.42415, 556383, scalability-analyzed, icsoft20}. This is, however, extremely sensitive to subtle design choices and a poor implementation can easily destroy the delicate superlinear scaling trend (see \S\ref{sec:background}).

Despite that superlinearity is genuinely measured \cite{scalability-analyzed, sdn-analytitcs, 6483679, 10.1007/978-3-319-77610-1, 10.1145/3627703.3629574, icsoft20} and thoroughly dissected \cite{dobb-1, dobb-2, 10.1145/2773212.2789974, 556383, 7733347, 80148} in piecemeal applications, currently there is a lack of a general architectural blueprint that would help system architects to methodologically \emph{engineer distributed systems towards superlinear scaling}. In this paper we aim to fill this gap. Our motivation is that networking applications are often embarrassingly parallel with little or no dependency between threads, promising massive (superlinear) parallel execution gains.

We observe that in order to achieve superlinearity one has to carefully combine an appropriate load balancing policy with a proper worker implementation. Indeed, load balancing in distributed systems is often non-arbitrary: web apps apply the ``sticky sessions'' rule to route all requests of a particular user to the same web server; %, rendering subsequent requests faster by having all per-user state available locally;
networking code commonly uses IP 5-tuple hashing at the NIC to ensure that all packets of a flow are processed on the same CPU; % that has local access to per-flow information;
and key-hashing in sharded key-value stores concentrates queries to a key at the same replica. Such policies tend to make the input stream processed by the parallel workers more predictable, compared to the aggregate input processed by the system. Combining such a \emph{locality boosting load balancer} with a \emph{self-adjusting algorithm} so that workers can take advantage of the higher input predictability to adaptively improve their own performance will, as we show both theoretically and empirically, yield faster-than-linear speedup in a broad range of applications.

The power of the resultant \emph{distributed self-adjusting system architecture} we advocate in this paper (\S\ref{sec:architecture}) is \emph{not} that it confirms the existence of superlinear scaling (this has been known for a while \cite{dobb-1, dobb-2}), neither that it would defy well-established scaling laws (it does not, see \cite{80148, gunther-hotsos, 10.1016/0167-8191(86)90024-4,10.1145/2773212.2789974}) nor that it produces the most efficient implementations possible (e.g., our packet classifier will not be as efficient as, say, a DPDK equivalent \cite{rte-acl} just by the fact that it runs inside the Linux kernel \cite{295475}). Rather, our main contribution is that we precisely \emph{identify the main architectural ingredients}, locality boosting and self-adjustment, \emph{which in combination allow a distributed system to scale faster than linear}. Our system architecture will then provide a mental model that guides us in re-engineering several commonly used distributed systems with surprisingly little effort to attain real and tangible performance improvement, often in the range of several orders of magnitude as evidenced by our subsequent case studies.

First we confirm the viability of our system architecture in extensive simulations. Deploying well-known list and tree search algorithms from the literature % using our architectural blueprint
we achieve $100$--$3300\times$ speedup on 48 CPU cores, orders of magnitude surpassing plain, linear scaling. We support our empirical findings with a formal analysis and obtain a new generic scaling law for distributed self-adjusting systems (Appendix~\ref{sec:analytical-findings}). Then we present two fully operational case studies. As a major contribution we re-engineer the packet classifier built into the popular Linux kernel to reach superlinear scaling (\S\ref{sec:dist-classifier}). On synthetic and real-life firewall traces, our implementation exhibits up to $800\times$ speedup with 32 CPU cores, $5$--$25\times$ improvement beyond the default Linux firewall implementation which scales only (sub)linearly. We also apply our methodology to a combined Memcached+PostgreSQL storage system, yielding $2.3\times$ faster than linear scaling (discussions moved to Appendix~\ref{sec:dist-caching} for space reasons). We finally review related work (\S\ref{sec:related-work}) and summarize the guidelines for designing and implementing superlinearly scaling distributed systems (\S\ref{sec:conclusions}). All code will be available as open source after publication. This work raises no ethical concerns. 


% The maximum speedup, measured as the ratio of the wall clock times of sequential and parallel execution, is formally described by Amdahl's law \cite{10.1145/1465482.1465560}. In general, the greater the sequential portion compared to the parallelizable fraction of the code, the more performance is lost compared to an ``ideal'' linear scaling, and the faster the system reaches saturation (see Fig.~\ref{fig:amdahl}). Amdahl's law has remained one of the most useful tools in the system engineering toolbox throughout the almost 60 years since its first publication \cite{10.5555/1951599, 10.1145/42411.42415, 4563876, 6280307,1580395,406581,6163449, 10.5555/775339.775386}.

% , e.g., database systems \cite{scalability-analyzed, 10.5555/1012889.1012894}, distributed caching \cite{271208, dobb-2}, SDN analytics \cite{sdn-analytitcs}, high-performance computing \cite{556383, 7733347, 6483679}, multi-robot systems \cite{10.1007/978-3-319-77610-1}, information retrieval systems \cite{dobb-1, dobb-2}, and large-scale network simulations \cite{10.1145/3627703.3629574} (see full taxonomies in \cite{7733347, 80148}).

% Many authors argue, however, that superlinear scaling is merely a~byproduct of running memory-\slash cache-bound applications on a ``bigger machine'' \cite{80148}, others are concerned that it is hard to generalize beyond a specific set use cases \cite{7733347, 80148}, and some outright dismiss faster-than-linear scaling all together \cite{gunther-hotsos, 10.1016/0167-8191(86)90024-4}, concluding that \emph{``superlinearity, although alluring, is as illusory as perpetual motion''} \cite{10.1145/2773212.2789974}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:



