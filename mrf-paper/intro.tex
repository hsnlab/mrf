\section{Introduction}\label{sec:introduction}


% use cases: multi-threaded app (RSS(HW) or RPS (SW)), cloud + cluster, job queues

% moore

% embarrassingly parallel

% amdahl limits

% superlinearity observed + perpeetum motion

% we show a systematic way to architect distributed systems to achieve SL scaling: LB-LB + SA

% sims + case study - results (360x in sims, 100x in benchmarks over Linux default)

With the end of Moore's law, computing power in modern computing systems increasingly comes in the form of parallel processing resources.  A major obstacle faced by network engineers is how to harness this increasingly parallel computing power \cite{265065, 10.5555/3307441.3307467, 10.1145/2815400.2815423, 10.1145/3098822.3098826, 10.5555/3154630.3154639}.

In horizontally scaled applications, a load balancer dispatches jobs across a fleet of workers that process the jobs in parallel \cite{10.5555/3235491}.  In the context of \emph{web applications}, HTTP load balancers \cite{194966, 211279, 9552525} distribute requests across a swarm of backend web servers.  % by hashing over the source IP address (``sticky sessions'').
% This way, requests from the same client will hit the same backend server, improving request locality.
% Meanwhile, resource state is maintained in a key-value store or a relational database.
Multicore \emph{OS network stacks} \cite{211263, 10.1145/3359989.3365412, 10.1145/3452296.3472914} run multiple instances of the networking logic on different CPUs and leverage the NIC to dispatch packets to CPU cores. % In order to avoid packet reordering and improve CPU cache performance, the NIC typically computes a hash over the packet header fields to select the CPU (RSS, RPS, etc.).
In massive-scale \emph{key-value stores} \cite{ghigoff2021bmc}, the key-space is hashed into multiple shards (partitions) and each shard is assigned to a separate server for processing.  The system's overall goal is to achieve the greatest possible parallel speedup with a given number of workers, in terms of the the lowest time for executing a single task or the most tasks that can be finished during a given time period.

Suppose a web app handles 100 requests per second using a single server. As we add another server, we expect a throughput of 200 requests per second. In reality, however, we usually obtain slightly less, and this is worsened as the system is scaled up further. This is because some fraction of most workloads is inherently sequential and, therefore, bound to execute on a single CPU core. For instance, the web servers may need to synchronize on a mutex to access global state, rendering all state updates sequential.  Beyond a certain threshold parallel performance plateaus due to the sequential workload dominating the running time and becoming a bottleneck.

The maximum speedup, measured as the ratio of the wall clock times of sequential and parallel execution, is formally described by Amdahl's law \cite{10.1145/1465482.1465560}. In general, the greater the serial portion compared to the parallelizable fraction of the code, the more performance is lost compared to an ``ideal'' linear scaling and the faster the system reaches saturation (see Fig.~\ref{fig:amdahl}). Amdahl's law is a cornerstone result in the parallel and high-performance computing practice and, despite often being debated \cite{10.1145/42411.42415}, extended \cite{4563876, 6280307,1580395,406581,6163449}, and misused \cite{10.5555/775339.775386}, it has remained one of the most useful tools in the system engineering toolbox \cite{10.5555/1951599}.

\begin{figure}[t]
  \centering
  % \includegraphics[width=0.8\linewidth]{fig/usl.png}
  \begin{small}
  \input{fig/usl.tex}
\end{small}
  \caption{Linear scaling, Amdahl's law and the superlinear scaling ($s=0.01$). The inset shows the asymptotics.}
  \label{fig:amdahl}
\end{figure}

Inherent to Amdahl's law is that no system can scale faster than linear: doubling parallel resources will yield at most two times the performance. Curiously, there have been several reports on faster-than-linear (\emph{superlinear}) scaling experimentally observed in, e.g., database systems \cite{scalability-analyzed, 10.5555/1012889.1012894}, SDN analytics \cite{sdn-analytitcs}, high-performance computing \cite{556383, 7733347, 6483679}, multi-robot systems \cite{10.1007/978-3-319-77610-1}, and parallel search in information retrieval systems \cite{dobb-1, dobb-2} (see full taxonomies in \cite{7733347, 80148}).
% There seem to be two ways to achieve such superlinear speedup \cite{7733347, 80148}: do disproportionately less work in each worker as we scale the system \cite{7733347}, or add more resources per thread \cite{80148}.
One typical context in which superlinear growth often emerges is distributed caching \cite{271208, 10.5555/1012889.1012894, dobb-2}: the more CPU cores the more (unshared) L1 cache space available to the application, which tends to make memory-bound\slash cache-bound code disproportionately faster \cite{80148} (see an analysis in \S\ref{sec:background}).  Many authors argue, however, that this is ``cheating'' \cite{gunther-hotsos, 10.1145/2773212.2789974} since the system now runs on a ``bigger machine'' \cite{80148}, others are concerned that superlinear scaling is hard to generalize beyond a specific set use cases \cite{7733347, 80148}, and some outright dismiss it all together \cite{gunther-hotsos, 10.1016/0167-8191(86)90024-4}, concluding that \emph{``superlinearity, although alluring, is as illusory as perpetual motion''} \cite{10.1145/2773212.2789974}.

In this paper we present the first general architecture to systematically engineer distributed systems for superlinear scaling. Our motivation is that distributed systems, and especially networking applications, are often embarrassingly parallel, % with little or no dependency between parallel workers,
which may admit a massive superlinear initial growth phase before scaling eventually and unavoidably blocking on a serial bottleneck.

Our main observation is that, to achieve superlinearity, one has to carefully combine an appropriate load-balancing policy with a proper worker implementation. Indeed, load balancing in distributed systems is non-arbitrary: web apps apply the ``sticky sessions'' rule to route all requests of a particular user to the same web server, rendering subsequent requests faster by having all per-user state available locally; networking code often uses IP 5-tuple hashing on the NIC, ensuring that all packets of a flow are processed on the same CPU that has local access to per-flow information; and key-hashing in sharded key-value stores assigns client requests to specific replicas that have exclusive local access to the relevant potion of the key-space. Each of these load balancing policies \emph{boost the locality of reference} in the input streams processed by the parallel workers. This, paired with a \emph{self-adjusting algorithm}, so that workers can take advantage of the higher input locality to improve their performance adaptively, will, as we show both theoretically and empirically, yield faster-than-linear speedup in a broad range of applications (see Fig.~\ref{fig:amdahl}). % One growth factor would come from the self-adjusting algorithm becoming proportionately faster as it processes a smaller and smaller subset of the inputs, and another factor would result from the fact that we throw more CPU resources to the system.

After some necessary background on Amdahl's law (\S\ref{sec:background}), we present our \emph{distributed self-adjusting system architecture} and show that superlinear speedup is a natural product of combining locality-boosting load-balancing with self-adjusting algorithms (\S\ref{sec:architecture}). Using common list and tree search algorithms from the literature, we achieve 100x--1000x speedup in simulations, orders of magnitude surpassing Amdahl's law or even plain, linear scaling. As an unexpected byproduct, we attain linear scaling even when we limit the system to a single CPU core. Then we extend our analysis to real systems (\S\ref{sec:case-study}): we carefully apply our methodology to engineer a Linux-kernel based packet classifier implementation to reach superlinear scaling. On synthetic and real-life firewall traces, our implementation shows up to 80x faster than linear scaling, 4.5x--6.3x improvement beyond the default Linux firewall implementation which scales according to Amdahl's law. Finally, we review related work (\S\ref{sec:related-work}) and draw the conclusions (\S\ref{sec:conclusions}). We note that all code will be available as open source after publication.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

