% !TEX ROOT = ./distributed_mrf.tex
\section{Background}\label{sec:background}

\begin{figure}[t]
  \centering
  \begin{small}
    \input{fig/usl.tex}
  \end{small}
  \caption{Linear scaling, Amdahl's law and superlinear scaling ($s=0.01$). The inset shows the asymptotics.}
    \label{fig:amdahl}
\end{figure}

There is an extensive background on scaling laws for characterizing the performance of a parallel system as the function of the computing\slash storage capacity available to it. In the following, we will use the terms ``distributed'' and ``parallel'' interchangeably to connote a networked system scaled to multiple independent compute threads (``workers''), e.g., scheduled to parallel CPUs of the same node, distributed to separate nodes, run in multiple datacenters, etc.

A cornerstone result in parallel computing, Amdahl's law \cite{10.1145/1465482.1465560} establishes a firm limit on the performance gain one can obtain by distributing a computation task over multiple processors. Given a partially parallel program, denote the fraction of execution time spent in the sequential part of the code by $s$, and the parallel fraction by $(1-s)$. Here, some code is ``sequential'' if it cannot benefit from the improvement of parallel computing resources, like single-threaded code, critical sections guarded by exclusion locks, etc. Denote by $T(k)$ the runtime (in seconds) of the program when executed on $k$ processors, and let $S(k)=\frac{T(1)}{T(k)}$ denote the performance improvement relative to a single-threaded execution (i.e., the \emph{speedup}). Then, the following holds (see Fig.~\ref{fig:amdahl}):
\begin{equation}\label{eq:amdahl}
S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{k}} \enspace .
\end{equation}

Here, $\frac{1-s}{k}$ establishes that the perfectly parallel part of the program executes $k$ times faster on $k$ processors than on a single core. By Amdahl's law, \emph{(i)} no code can scale faster than linear (i.e., $\frac{d S(k)}{d k} \le 1$, with equality exactly when $s=0$), \emph{(ii)} throwing additional workers on a computation task yields diminishing returns ($\frac{d S(k)}{d k}$ is monotonically decreasing in $k$) and \emph{(iii)} the asymptotics is limited by the sequential part only ($\lim_{k\to \infty}S(k) = \frac1{s}$). For different applications and extensions of Amdahl's law, see \cite{4563876, 6280307,1580395,406581,6163449, 10.5555/1951599}.

Curiously, there have been several reports from a broad range of applications indicating faster-than-linear scaling, e.g., database systems \cite{scalability-analyzed, 10.5555/1012889.1012894}, distributed storage systems \cite{271208, dobb-2, icsoft20}, SDN analytics \cite{sdn-analytitcs}, high-performance computing applications \cite{556383, 7733347, 6483679}, multi-robot systems \cite{10.1007/978-3-319-77610-1}, information retrieval systems \cite{dobb-1, dobb-2}, and large-scale network simulations \cite{10.1145/3627703.3629574} (see full taxonomies in \cite{7733347, 80148}). % (Note that in the majority of the literature any function growing faster than $f(x) = x$ is considered ``superlinear'', despite that, e.g., $f(x) = 3x$ is, mathematically, linear. Some authors distinguish these functions using the term ``superunitary'' \cite{80148}. In line with the literature we will use the former terminology below.)
One way to reconcile these empirical observations and Amdahl's law is the \emph{scaled size model} \cite{556383}. Critical to Amdahl's law is the assumption that the size of workers' sub-problems remains constant as we scale the system \cite{10.1145/42411.42415}. Under this \emph{fixed size} assumption \cite{556383}, faster-than-linear scaling is impossible \cite{10.1016/0167-8191(86)90024-4}. However, when this assumption fails, say, when the workers' jobs get progressively smaller or execution gets gradually faster as we add more parallel workers (scaled size model), superlinear scaling often emerges \cite{scalability-analyzed, sdn-analytitcs, 6483679, 10.1007/978-3-319-77610-1}.

Sometimes faster-than-linear growth appears almost accidentally. Imagine a naive parallel dense matrix-multiplication algorithm that factors input matrices into multiple blocks, performs the multiplication of the blocks in parallel, and aggregates the results \cite{7733347}. Easily, blocks will get smaller as we add more processors, so that after a certain point the entire input of workers will fit into CPU fast cache, yielding a disproportionately faster parallel execution. Conditions under which such superlinear (or ``super-unitary'' to be absolutely precise \cite{80148}) scaling emerges are widely discussed \cite{556383, dobb-1, dobb-2}, analyzed \cite{80148, 7733347}, and debated \cite{gunther-hotsos, 10.1016/0167-8191(86)90024-4, 10.1145/2773212.2789974}. What is missing is a generic design methodology to \emph{engineer} distributed systems for superlinear scaling. Such a model would also help identify the cases when superlinear scaling is possible, and when it is not. 
Our main contribution is to highlight a design pattern that yields superlinear scaling.

% Many authors argue, however, that superlinear scaling is merely a~byproduct of running memory-\slash cache-bound applications on a ``bigger machine'' \cite{80148}, others are concerned that it is hard to generalize beyond a specific set use cases \cite{7733347, 80148}, and some outright dismiss faster-than-linear scaling all together \cite{gunther-hotsos, 10.1016/0167-8191(86)90024-4}, concluding that \emph{``superlinearity, although alluring, is as illusory as perpetual motion''} \cite{10.1145/2773212.2789974}.


% Currently the only general methodology to achieve faster-than-linear scaling seems to require deploying additional fast caches. Moving an application to a ``bigger machine'' \cite{dobb-2}, however, is not always feasible due to, e.g., physical or financial constraints.  In some cases caching cannot be used at all (e.g., for inherently stateful computations or complex database queries) or it introduces more overhead than it saves (e.g., for predominantly uniform input or rapidly changing data).  Moreover, caching comes with certain extra complexity and often cache invalidation and eviction policies and data consistency mechanisms are too costly to implement in a massively distributed setting \cite{271208}. Apart from caching, however, currently the only way to achieve faster-than-linear scaling is to rely on piecemeal problem-specific techniques, comprehensive domain knowledge, and pure luck \cite{7733347, 80148}. And even then, some compellingly argue that superlinear growth itself is a performance illusion, which goes against the very laws of nature much like perpetual motion \cite{gunther-hotsos, 10.1145/2773212.2789974}

% Many authors argue, however, that superlinear scaling is merely a~byproduct of running memory-\slash cache-bound applications on a ``bigger machine'' \cite{80148}, others are concerned that it is hard to generalize beyond a specific set use cases \cite{7733347, 80148}, and some outright dismiss faster-than-linear scaling all together \cite{gunther-hotsos, 10.1016/0167-8191(86)90024-4}, concluding that \emph{``superlinearity, although alluring, is as illusory as perpetual motion''} \cite{10.1145/2773212.2789974}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

