\section{Background}\label{sec:background}

First we review Amdahl's scaling law and then we show a typical pattern to surpass it: distributed caching.

\subsection{Amdahl's Law}
\label{sec:amdahl-law}

A cornerstone result in parallel computing, Amdahl's law \cite{10.1145/1465482.1465560} establishes a firm limit on the performance gain one can obtain by distributing a computation task over multiple processors. Given a partially parallel program, denote the fraction of execution time spent % by a single-threaded execution
% by the processor 
in the serial part of the code by $s$, and the parallel fraction by $(1-s)$. Here, some code is ``serial'' if it cannot benefit from the improvement of the computing resources, like single-threaded code, critical sections guarded by exclusion locks, etc. Denote by $T(k)$ the runtime (in seconds) of the program when executed on $k$ processors, and let $S(k)=\frac{T(1)}{T(k)}$ denote the performance improvement relative to a single-threaded execution (i.e., the \emph{speedup}). Then, the following relation holds (see Fig.~\ref{fig:amdahl}):
\begin{equation}\label{eq:amdahl}
S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{k}} \enspace .
\end{equation}

Here, the term $\frac{1-s}{k}$ establishes that the perfectly parallel part of the program executes $k$ times faster on $k$ processors than on a single core. By Amdahl's law, (1) no code can scale faster than linear (i.e., $\frac{d S(k)}{d k} \le 1$, with equality exactly when $s=0$), (2) throwing additional processors on a computation task yields diminishing returns ($\frac{d S(k)}{d k}$ is decreasing in $k$) and (3) the asymptotics is limited by the serial part only ($\lim_{k\to \infty}S(k) = \frac1{s}$). 

\subsection{Superlinear speedup}
\label{sec:backgound-superlinear}

% The uniting idea of both parallel computing and multi-robot systems is that having multi-
% ple processors or robots working on a task decreases the processing time. Typically we desire
% a linear speedup, that is, doubling the number of processing units halves the execution time.
% Sometimes superlinear scalability is observed in parallel computing systems and more fre-
% quently in multi-robot and swarm systems. Superlinearity means each individual processing
% unit gets more efficient by increasing the system size—a desired and rather counterintuitive
% phenomenon.

% As superlinear speedups seem special, they were frequently discussed and studied [2, 3]. There
% even exists a proof showing the impossibility of superlinear speedups but it assumes fixed problem
% size [4]. Superlinear speedups are rather infrequently observed in parallel computing (e.g., cache-
% size effects [5]) compared to rather frequent observations in multi-robot and swarm systems (e.g.,
% inherently collaborative tasks [6]). When observed, superlinearity is often a discrete effect, such
% as a workpackage happening to fit into the processors cache [5] or a robot group being able to
% form a bucket brigade [7, 8]. Superlinear scalability has much potential that should be enough
% motivation to investigate it across different domains and to understand how one can provoke it.


% THE systems goalis of to today’s achieve world the greatest of parallel speedup, and represented
% distributed either as the lowest time for execution of a single task (High Performance Computing),
% or to execute as many tasks as possible for a given time (High Throughput Computing), when the
% task(s) are executed on scaled resources. Many new algorithms and computing paradigms appeared in
% the last decade, and new challenges have emerged to solve more complex problems faster, or to
% achieve greater speedup, as much as possible [1].  The speedup is usually defined as a ratio of the
% wall times of sequential and parallel execution of an algorithm. The target of the parallelization
% is to achieve the lowest execution time in order to maximize the speedup against the best
% sequential algorithm. Increasing the number of computing resources will increase the
% intra-resource’s communication and requires additional operations, such as reduction operations.


% Many authors reported the existence of a superlinear
% speedup, but most of them only mentioned it as a side effect
% [4]. Besides reporting a superlinearity, other researchers briefly
% presented that the reason for achieving a superlinear speedup
% is because of the greater amount of cache memory in the
% parallel execution compared to the sequential [5].

Inherent to Amdahl's law is the ``fixed size'' assumption \cite{556383}, namely that the size of the sub-problems assigned to the workers remains constant as we scale the system to more parallel workers (see also ``Gustafson's law'' \cite{10.1145/42411.42415}). Several have observed, however, that this assumption does not always hold \cite{80148,556383}. This can happen, for instance, due to an intricate interplay between problem size per worker and available memory. Sometimes, superlinear scaling\footnote{Many authors consider any function growing faster than $f(x) = x$ to be ``superlinear'', despite that, for instance, $f(x) = 3x$ is still linear mathematically. Others distinguish such functions using the term ``superunitary'' \cite{80148}. In line with the literature we will use the term ``superlinear'' in the former meaning.} emerges because the problem can be partitioned into smaller chunks that fit the CPU cache at the workers, thus reducing cache misses \cite{7733347}. Many networking applications explicitly contain a fast-path\slash cache; e.g., distributed caches (e.g., \texttt{memcached}) are often used as a fast cache for a ``slow'' web service, popular keys are cached in the OS kernel for fast key-value stores \cite{179747, ghigoff2021bmc}, FIB caches maintain the most frequent IP routes to sidestep sluggish longest prefix matching \cite{rottenstreich2016optimal}, hierarchical flow caches serve as a fast-path in programmable software switches \cite{188960}, etc. All these workloads may benefit from the caches becoming more efficient as the system is scaled, yielding faster-than-linear speedup. % Note, however, that this only happens if the requests are dispatched carefully (see later) so that each worker processes only a small subset of all possible inputs, otherwise cache efficiency will not improve.

\begin{figure}
  \centering
  \begin{small}
    \input{fig/cache/cache_analysis.tex}
\end{small}
\caption{Scaling laws for distributed caching: hash-based load balancing, lower envelope (round robin load balancing) and upper envelope (perfect cache hit rate with $k$ caches). Parameters: $s=0.1$, $\delta=0.1$ and $\rho=10$.}
  \label{fig:dcache-analysis}
\end{figure}

% refer to "Modeling Speedup (n) Greater than n" -> analysis

% assumption for the analysis? ``I think equation 2 should be explained much better. It is not at all obvious (not sure even correct) that hit rate scales linearly with threads (I think only true is delta << 1). What is $\rho$ exactly the ratio of (fetching time in the event of miss) / (fetching time in the event of a catch hit)?''

It is instructive to quantify this mechanism using a simple model. Suppose a source emits uniformly distributed random requests for $m$ items and requests are distributed among $k$ workers by hashing on the request id. This effectively partitions the requests into $k$ random buckets, so that each worker will perceive uniformly distributed requests for only $\frac{m}{k}$ items.  Suppose that initially the cache hit rate for a single worker, which processes all $m$ requests, is $\delta$. Then, scaling the system to $k$ workers will improve the cache hit rate to $k\delta$ ($k\delta \le 1$). This puts the lookup time of the system of $k$ parallel caches to
\begin{align}\label{eq:dist-cache}
  T_c(k) = \begin{cases} s + \frac{1-s}{k}(k\delta + (1-k\delta)\rho) & \text{if } k\delta \le 1\\s + \frac{(1-s)}{k} & \text{otherwise}\end{cases} \enspace ,
\end{align}
where $\rho$ is the penalty for a cache miss event.

The speedup $S_c(k)=\frac{T_c(1)}{T_c(k)}$ is depicted in Fig.\ref{fig:dcache-analysis}. The lower envelope of the scaling profile is given by Amdahl's law for the system with round-robin load-balancing ($\frac{T_c(1)}{s + \frac{1-s}{k}(\delta + (1-\delta)\rho)}$). As $k$ grows the scaling profile progresses over a superlinear curve to an elevated Amdahl's law profile, representative of a system where \emph{all} $\frac{m}{k}$ requests are served be workers from fast memory ($\frac{T_c(1)}{s + \frac{1-s}{k}}$). Note that this occurs \emph{only} if request dispatching is chosen carefully to partition the item space. For instance, modulo hashing achieves this but a roundrobin load balancer does not, as it may send any item to any worker, which defeats the purpose of improving request locality at the workers and destroys superlinear scaling all together  (see empirical evidence in the next Section).

% there is an actual conclusion in `` SUPERLINEAR SPEEDUP IN HPC SYSTEMS: WHY AND WHEN?``: ``Mainly the superlinear speedup performance in persistent algorithms occurs due to the increased cache re- sources in the parallel computer architectures, the prefetching of shared variables in shared memory organization, or better scheduling in heterogeneous environments.''

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

