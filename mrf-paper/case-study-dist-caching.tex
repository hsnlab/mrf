% !TEX ROOT = ./distributed_mrf.tex
\section{Superlinear scaling in distributed caching}\label{sec:dist-caching}

Superlinear scaling often emerges in systems where a ``fast'' \emph{distributed cache} is deployed in front of ``slow'' processing system or storage engine \cite{scalability-analyzed, sdn-analytitcs, dobb-2}.  Examples include multi-processor CPUs with unshared Level-1 fast cache memory that make access to program arguments more efficient \cite{7733347}, runtimes that selectively ``memoize'' the results of costly computations \cite{10.1109/TC.2005.119}, FIB caches in OS network stacks that maintain the most recent IP routes in fast memory to sidestep longest prefix matching \cite{rottenstreich2016optimal}, hierarchical (mega)flow caches that serve as a fast-path in programmable software switches \cite{188960}, etc. All these workloads may benefit from caches becoming more efficient as the system is scaled and, potentially, show superlinear speedup on certain workloads. Below we reproduce this finding using Memcached as a fast cache for the PostgreSQL database management system \cite{180324,10.5555/1012889.1012894, 179747, ghigoff2021bmc}.

\begin{figure*}[t]
  \begin{minipage}[t]{.35\linewidth}
    \centering%
    \input{fig/cache/cache_analysis.tex}%
    \caption{Scaling laws for distributed caching.}%: hash-based load balancing, lower envelope (round robin load balancing) and upper envelope (perfect cache hit rate with $k$ caches).}%
    \label{fig:dcache-analysis}%
  \end{minipage}
  \hspace{10pt}%
  % \hfill%
  \begin{minipage}[t]{.6\linewidth}
    \input{fig/dist-cache-speedup.tex}%
    \caption{Results for a joint scaling of Memcached+PostgreSQL with and without key-hashing: speedup and cache-hit rate.}%
    \label{fig:dist-cache-speedup}%
  \end{minipage}
\end{figure*}

It is instructive to quantify superlinear speedup in this context using a simple model. Suppose a source emits uniformly distributed random requests for $m$ items and requests are distributed among $k$ workers, each using a separate cache of size $c$, by hashing on the request id.  Initially, the cache hit rate for a single worker that processes all $m$ possible requests is $\delta := \sfrac{c}{m}$. Adding $k$ workers effectively partitions the requests into $k$ random buckets so that each worker will perceive uniformly distributed requests for only $\sfrac{m}{k}$ items, which improves the cache hit rate at each worker to $\frac{c}{\sfrac{m}{k}} = k\delta$ ($k\delta \le 1$). This puts the lookup time of the system of $k$ parallel caches to
\begin{align}\label{eq:dist-cache}
  T_c(k) = \begin{cases} s + \frac{1-s}{k}(k\delta + (1-k\delta)\rho) & \text{if } k\delta \le 1\\s + \frac{(1-s)}{k} & \text{otherwise}\end{cases} \enspace ,
\end{align}
where $\delta$ is the single-threaded cache hit rate, $\rho$ is the penalty for a cache miss, and $s$ denotes the fraction of execution time spent in the sequential part of the code.

The speedup $S_c(k)=\frac{T_c(1)}{T_c(k)}$ for the parameters $s=0.1$, $\delta=0.1$ and $\rho=10$ is depicted in Fig.~\ref{fig:dcache-analysis}. The lower envelope of the scaling profile is given by Amdahl's law for the system with random or round robin load-balancing. % ($\frac{T_c(1)}{s + \frac{1-s}{k}(\delta + (1-\delta)\rho)}$).
As $k$ grows the scaling profile progresses over a superlinear curve to an elevated Amdahl's law profile, representative of a system serving \emph{all} requests from fast memory. % ($\frac{T_c(1)}{s + \frac{1-s}{k}}$).
Note that this occurs \emph{only} if request dispatching is chosen carefully to partition the item space. Modulo hashing assigns the same item to the same worker deterministically, so that workers process only a subset of the items that may have a greater chance to fit into the cache. In contrast, a random or a round robin load balancer may assign any item to any worker, which defeats the purpose of improving workers' cache hit rate. % and destroys superlinear scaling all together. % (see empirical evidence in the next Section).

Note that for a system to match this scaling profile the fast cache and the slow backend must be scaled jointly. In the below we will use this setup, by increasing the number of Memcached instances and PostgreSQL client threads \emph{simultaneously}. The version where \emph{only} the cache is scaled while the backend runs with constant CPU resources would match a different scaling profile characterized by $T_c(k) = s + (1-s)(\delta + (1-k\delta)\rho), k\delta \le 1$. This also produces superlinear speedup, just with a slower ramp-up.

In our case study Memcached is gradually scaled from a single replica to 15 replicas, playing the role of a distributed cache for a ``slow'' PostgreSQL v14 database. PostgreSQL is scaled proportionally to the number of Memcached replicas; in particular we run 4 PostgreSQL client threads per cache replica. Note that each Memcached replica runs on a different port letting clients to address each one separately, which is critical to implement key-hashing. We wrote a custom multi-threaded client that performs a configurable number of cache-aside read iterations: first it tries to read from Memcached and, on a cache miss, makes a read from PostgreSQL and writes the result back into the cache. We used two different load balancing schemes to route key requests: random load balancing reads from a random Memcached replica, while key-hashing always reads\slash writes the same key from\slash to the same Memcached replica.

Fig.~\ref{fig:dist-cache-speedup} shows the results for PostgreSQL pre-filled with 1,000,000 key-value pairs of 16 byte keys and 48 byte values, with a configurable number of PostgreSQL threads and Memcached replicas with 4 MB of cache each. As expected, superlinear scaling emerges with key-hashing, yielding $35\times$ speedup with $15$ Memcached replicas, $2.3\times$ higher than linear scaling. In contrast, random request routing exhibits only linear scaling. The reason is the improving cache hit rate as Memcached is scaled: with 14 replicas we reach close to 100\% cache hit rate and speedup falls back into the linear range, as predicted by the analysis.

We note that superlinear scaling in this context is extremely sensitive to certain benchmark parameters, like the number of Memcached replicas, PostgreSQL threads, and client threads. This is because for faster-than-linear scaling to appear Memcached replicas must be both CPU-bound (so that adding more replicas will improve throughput) \emph{and} memory-bounded (so that improving cache hit rate will cause speedup) at the same time. Earlier reports indicate that this occurs surprisingly commonly in practice \cite{dobb-1, dobb-2, 7733347, 80148, 556383, scalability-analyzed}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf.nsdi"
%%% End:
