
\subsection{Evaluation}
\label{sec:sims}

Fig.~\ref{fig:dist-self-adjusting-eval} presents the results from a comprehensive simulation study we conducted to understand distributed self-adjusting systems performance over a broad selection of load balancing policies, self-adjusting algorithms, and input distributions. The simulator was coded in roughly 1,000 lines of Go and uses lightweight threads (goroutines) managed by the Go runtime to run a given number of workers in parallel. We used a simple home-grown implementation for static and MTF lists and standard Go modules for LRU caches \cite{golang-lru}, static balanced trees \cite{golang-btree} and splay trees \cite{golang-splay}. In order to make tree lookup CPU bounded we used an ``expensive'' order underneath the tree, where every comparison operation costs a configurable $w$ number of extra cycles. The simulator creates the specified combination of a load balancer, $k$ worker threads running the selected lookup algorithm, and a random input sequence with a given request distribution, and then performs a configurable number of lookup operations and measures the total execution time with nanosecond precision. To obtain a full picture, the total execution time includes the transient time needed to warm up the self-adjusting algorithms running in the threads. For the specification of the evaluation platform, refer to \S\ref{sec:sa-nf-tables-eval}.

Our observations are as follows. First, it is immediate that \emph{the right combination of a locality-boosting load balancer and a self-adjusting algorithm robustly delivers superlinear speedup}, irrespectively of the problem domain or the input distribution. Even for a worst-case uniform input we obtain $3,300\times$ speedup for list access on 48 CPU cores, almost $70\times$ of ``ideal'' linear speedup, $200\times$ speedup on LRU caches and $65\times$ speedup on tree search with 36 CPU cores. Usually the superlinear growth is so dominant that we can hardly put Amdahl's scaling on the same diagram.  Second, \emph{only the combination of locality-boosting load balancing and self-adjusting algorithms produces superlinear speedup}, all other combinations (i.e., round robin with any algorithm or static algorithm with any load balancer) fall back to Amdahl's scaling.  Third, \emph{self-adjustment clearly has its overhead}. This can be observed in Fig.~\ref{fig:singlecore-list-uniform}, which, instead of the relative speedup shows the absolute throughput. Here, the single-threaded self-adjusting version is clearly slower than the static version (a trait we identified in essentially all cases with uniform input). Fourth, \emph{the overhead of self-adjustment becomes irrelevant for more than one CPU core, or with skewed request distributions}. For instance on a Zipf input distribution (Fig.~\ref{fig:multicore-list-zipf}) even the single-threaded self-adjusting version is already $2$--$2.5\times$ faster in an absolute term irrespectively of the load balancer (not shown in the figure). However, \emph{only} combining with a locality-boosting load balancer it produces superlinear speedup.

And finally a rather surprising finding. In Fig.~\ref{fig:singlecore-list-uniform} we show an evaluation that was executed with an increasing number of parallel threads manually constrained to run with at most 110\% CPU utilization using \texttt{cpulimit}. This effectively simulates a single core worth of total CPU shared by \emph{all} the parallel workers, with a little surplus for the load balancer. The results indicate that the distributed self-adjusting system (but \emph{only} this combination!) delivers linear speedup with adding new threads that share a single core. % With uniformly distributed requests, we achieve $25\times$ speedup by spawning 25 parallel goroutines, each sharing a single CPU core.  
And this is despite that the overhead of request generation, goroutine scheduling, and memory management all count towards the total system load and take away precious CPU time from the useful workload.

But how can parallelization benefit performance when the total CPU power available to the system is kept constant? Recall, in the multicore case superlinear speedup emerges thanks to the superposition of two independent $k\times$ speedup trends, one delivered by the self-adjusting workers and another added by us throwing $k\times$ more CPU power to the system. When the total available CPU is limited only first $k\times$ speedup factor is in effect, resulting in the observed linear scaling trend.

% glitches are cpu architecture specific

% LET's SKIP THIS: highly speculative!!!!!!!!!!!1
%
% \subsection{Revised Amdahl's law}
% \label{sec:sims}

% An interpretation of superlinear scaling: if we introduce the notion of the ``virtual job size''. Implicit in Amdahl's law \eqref{eq:amdahl} is that the job size remains the same independently of $k$. Parallel self-adjustments, however, may actually \emph{decrease} the amount of work each worker has to perform per each request. Let $b(k)$ denote the ``virtual job size'' perceived by each worker when the number of  workers is $k$. We observe that in parallel self-adjusting systems $b(k)$ is decreasing in $k$; e.g., for MTF we have $b(k) = \frac1{k}$.

% \begin{equation}\label{eq:revised-amdahl}
% S(k) = \frac{T(1)}{T(k)} = \frac{1}{s + \frac{1-s}{k^{\alpha}}} \enspace .
% \end{equation}

% Amdah's law for $\alpha=1$, distributed MTF scaling for $\alpha=1$, superlinear scaling with $\alpha>1$

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "distributed_mrf"
%%% End:

